[{"authors":null,"categories":null,"content":"Hello there! I\u0026rsquo;m Aritra Biswas, an ML Engineer at AB InBev and an independent ML researcher. I have a passion for developing products that make analytics more accessible and user-friendly. Originally from Calcutta, I currently reside in Bangalore. When I\u0026rsquo;m not immersed in my work, you can find me indulging in my hobbies of reading, traveling, and blogging. Join me on this exciting journey of exploring the world of technology and data through my blog!\nDownload my resumÃ©.\n","date":1554595200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1554595200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://academic-demo.netlify.app/author/aritra-biswas/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/author/aritra-biswas/","section":"authors","summary":"Hello there! I\u0026rsquo;m Aritra Biswas, an ML Engineer at AB InBev and an independent ML researcher. I have a passion for developing products that make analytics more accessible and user-friendly. Originally from Calcutta, I currently reside in Bangalore. When I\u0026rsquo;m not immersed in my work, you can find me indulging in my hobbies of reading, traveling, and blogging. Join me on this exciting journey of exploring the world of technology and data through my blog!\n","tags":null,"title":"Aritra Biswas","type":"authors"},{"authors":null,"categories":null,"content":"Hello there! I\u0026rsquo;m Aritra Biswas, an ML Engineer at AB InBev and an independent ML researcher. I have a passion for developing products that make analytics more accessible and user-friendly. Originally from Calcutta, I currently reside in Bangalore. When I\u0026rsquo;m not immersed in my work, you can find me indulging in my hobbies of reading, traveling, and blogging. Join me on this exciting journey of exploring the world of technology and data through my blog!\nDownload my resumÃ©.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"4036ab9376e924db20b9e42fb1307811","permalink":"https://academic-demo.netlify.app/author/aritra-biswas/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/aritra-biswas/","section":"authors","summary":"Hello there! I\u0026rsquo;m Aritra Biswas, an ML Engineer at AB InBev and an independent ML researcher. I have a passion for developing products that make analytics more accessible and user-friendly. Originally from Calcutta, I currently reside in Bangalore. When I\u0026rsquo;m not immersed in my work, you can find me indulging in my hobbies of reading, traveling, and blogging. Join me on this exciting journey of exploring the world of technology and data through my blog!\n","tags":null,"title":"Aritra Biswas","type":"authors"},{"authors":null,"categories":null,"content":" Table of Contents What you will learn Program overview Courses in this program Meet your instructor FAQs What you will learn Fundamental Python programming skills Statistical concepts and how to apply them in practice Gain experience with the Scikit, including data visualization with Plotly and data wrangling with Pandas Program overview The demand for skilled data science practitioners is rapidly growing. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi.\nCourses in this program Python basics Build a foundation in Python. Visualization Learn how to visualize data with Plotly. Statistics Introduction to statistics for data science. Meet your instructor Aritra Biswas FAQs Are there prerequisites? There are no prerequisites for the first course.\nHow often do the courses run? Continuously, at your own pace.\nBegin the course ","date":1611446400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1611446400,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://academic-demo.netlify.app/courses/example/","publishdate":"2021-01-24T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"An example of using Wowchemy's Book layout for publishing online courses.","tags":null,"title":"ðŸ“Š Learn Data Science","type":"book"},{"authors":null,"categories":null,"content":"Build a foundation in Python.\n1-2 hours per week, for 8 weeks\nLearn Quiz What is the difference between lists and tuples? Lists\nLists are mutable - they can be changed Slower than tuples Syntax: a_list = [1, 2.0, 'Hello world'] Tuples\nTuples are immutable - they can\u0026rsquo;t be changed Tuples are faster than lists Syntax: a_tuple = (1, 2.0, 'Hello world') Is Python case-sensitive? Yes\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"17a31b92253d299002593b7491eedeea","permalink":"https://academic-demo.netlify.app/courses/example/python/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/python/","section":"courses","summary":"Build a foundation in Python.\n","tags":null,"title":"Python basics","type":"book"},{"authors":null,"categories":null,"content":"Learn how to visualize data with Plotly.\n1-2 hours per week, for 8 weeks\nLearn Quiz When is a heatmap useful? Lorem ipsum dolor sit amet, consectetur adipiscing elit.\nWrite Plotly code to render a bar chart import plotly.express as px data_canada = px.data.gapminder().query(\u0026quot;country == 'Canada'\u0026quot;) fig = px.bar(data_canada, x='year', y='pop') fig.show() ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"1b341b3479c8c6b1f807553b77e21b7c","permalink":"https://academic-demo.netlify.app/courses/example/visualization/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/visualization/","section":"courses","summary":"Learn how to visualize data with Plotly.\n","tags":null,"title":"Visualization","type":"book"},{"authors":null,"categories":null,"content":"Introduction to statistics for data science.\n1-2 hours per week, for 8 weeks\nLearn The general form of the normal probability density function is:\n$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} $$\nThe parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$. Quiz What is the parameter $\\mu$? The parameter $\\mu$ is the mean or expectation of the distribution.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"6f4078728d71b1b791d39f218bf2bdb1","permalink":"https://academic-demo.netlify.app/courses/example/stats/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/stats/","section":"courses","summary":"Introduction to statistics for data science.\n","tags":null,"title":"Statistics","type":"book"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://academic-demo.netlify.app/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":null,"categories":null,"content":"About Aritra Biswas This is a brief biography about Aritra Biswas.\n","date":1730332800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1730332800,"objectID":"6083a88ee3411b0d17ce02d738f69d47","permalink":"https://academic-demo.netlify.app/about/","publishdate":"2024-10-31T00:00:00Z","relpermalink":"/about/","section":"","summary":"About Aritra Biswas This is a brief biography about Aritra Biswas.\n","tags":null,"title":"About Me","type":"page"},{"authors":null,"categories":null,"content":"About Aritra Biswas This is the biography of Aritra Biswas.\n","date":1730332800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1730332800,"objectID":"6f04e4dcf4c78c0787a95bda8c00a8cb","permalink":"https://academic-demo.netlify.app/author/aritra-biswas/","publishdate":"2024-10-31T00:00:00Z","relpermalink":"/author/aritra-biswas/","section":"authors","summary":"About Aritra Biswas This is the biography of Aritra Biswas.\n","tags":null,"title":"Aritra Biswas","type":"authors"},{"authors":[],"categories":["programming"],"content":"As we discussed in previous post, in this series we will be working on different type of regression problem and will try to parse them as sklearn model objects. Here are we will be working with ExponentialSmoothing from statsmodels library. This is one of the most common time series model used in general. statsmodels is heavily influence by R and its convention. If you take a deep dive into it\u0026rsquo;s functionality of or formula based approach in case of GLM, it is quite evident including the summary function.\nHere in this post our objective will be to take this ExponentialSmoothing from statsmodels and build a model class which looks like sklearn model and the method signatures are same in nature so there is less learning curve involved when there is a context switch or in this case a user is trying to build different models.\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing from sklearn.base import BaseEstimator, RegressorMixin from sklearn.utils.validation import check_X_y, check_array, check_is_fitted from sklearn.metrics import mean_absolute_percentage_error import numpy as np import pandas as pd import inspect class HWTimeSeriesSkLearnWrapper(BaseEstimator, RegressorMixin): def __init__( self, trend=None, damped_trend=False, seasonal=None, seasonal_periods=None, use_boxcox=False, optimized=True, remove_bias=False, initialization_method = None, model=ExponentialSmoothing, ): args, _, _, values = inspect.getargvalues(inspect.currentframe()) values.pop(\u0026quot;self\u0026quot;) for arg, val in values.items(): setattr(self, arg, val) def fit(self, X, y = None): if not X.empty: self.X = X self.model_ = self.model( self.X, trend=self.trend, seasonal=self.seasonal, seasonal_periods=self.seasonal_periods, damped_trend=self.damped_trend, use_boxcox=self.use_boxcox, ) self.result_ = self.model_.fit( optimized=self.optimized, remove_bias=self.remove_bias ) self.level = self.result_.level self.trend = self.result_.trend self.season = self.result_.season self.fitted_values = self.result_.fittedvalues return self def predict(self, X, y = None): check_is_fitted(self, \u0026quot;result_\u0026quot;) self.df = self.result_.forecast(X.index.size).to_frame() self.df.columns = [f\u0026quot;predicted_{name}\u0026quot; for name in X.columns] self.df.index.name = X.index.name return self.df def score(self, X, y): y_true = X.to_numpy().flatten() y_hat = self.predict(X).to_numpy().flatten() return mean_absolute_percentage_error(y_true= y_true, y_pred=y_hat) As you can see in the above block, I have come up with this structure which can take all the args of __init__ and fit of ExponentialSmoothing while initializing the wrapper class HWTimeSeriesSkLearnWrapper. Note, these are not all the exhaustive set of parameter. You can take a look into the source code of the model class from statsmodels and figure out what are the other parameters which can be helpful for your context.\nargs, _, _, values = inspect.getargvalues(inspect.currentframe()) values.pop(\u0026quot;self\u0026quot;) for arg, val in values.items(): setattr(self, arg, val) The above chunk of the code will help to initialize arbitrary number of parameters for your model and in subsequent methods such as initialization or fit you can pass the parameter values from self. In the next block, I am downloading the data and creating a train test split. Note, in case of time series we need to remember two things,\nTrain test split can not be random There is no such concept of train and test data. Index is kind of your core feature along with meta features which can be derived from target. The catch here is we need to create train and test in a way it can look like sklearn model but under the hood it should be able to use the same datasets for building times series model using statsmodels or any other library you may use.\ndef get_and_split_airline_data(test_size = 0.2, date_col = \u0026quot;month\u0026quot;, date_format = \u0026quot;%Y-%m\u0026quot;, date_freq = \u0026quot;MS\u0026quot;): df = pd.read_csv(\u0026quot;https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv\u0026quot;) df.columns = df.columns.str.lower() if date_col in df.columns: df[date_col] = pd.to_datetime(df[date_col], format = date_format) df = df.sort_values(date_col).set_index(date_col) df.index.freq = date_freq train_size = int(df.shape[0] * (1 - test_size)) X_train = df.iloc[:train_size] X_test = df.iloc[train_size:] y_train = X_train.copy() y_test = X_test.copy() return X_train,X_test,y_train, y_test Finally, once we have the data we can use fit, predict and score to calculate the required values.\nX_train,X_test,y_train, y_test = get_and_split_airline_data() model = HWTimeSeriesSkLearnWrapper(trend = \u0026quot;add\u0026quot;, damped_trend = True, seasonal = \u0026quot;add\u0026quot;, seasonal_periods = 12) model.fit(X_train, y_train) model.predict(X_test) model.score(X_test, y_test) ","date":1671494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671511439,"objectID":"306b43df964f0105ab47bb177d7c3147","permalink":"https://academic-demo.netlify.app/post/exponential-smoothing-using-scikit-learn-wrapper-statsmodels/","publishdate":"2022-12-20T00:00:00Z","relpermalink":"/post/exponential-smoothing-using-scikit-learn-wrapper-statsmodels/","section":"post","summary":"As we discussed in previous post, in this series we will be working on different type of regression problem and will try to parse them as sklearn model objects. Here are we will be working with ExponentialSmoothing from statsmodels library. This is one of the most common time series model used in general. statsmodels is heavily influence by R and its convention. If you take a deep dive into it\u0026rsquo;s functionality of or formula based approach in case of GLM, it is quite evident including the summary function.\n","tags":["ML","python"],"title":"Exponential Smoothing using Scikit-Learn wrapper \u0026 statsmodels","type":"post"},{"authors":[],"categories":["programming"],"content":"Those who are working in ML space of sometime must be aware that there are multiple python libraries out there which support different algorithms/models. When you are trying to implement best model it is not possible to use just a single library or even a single language and compute infra. Also, note all the libraries are do not have the same method, signature and input and output. Hence if you are working on a library which is collection of multiple models from different library one good starting point can be to standardize you class, methods, signature and input and output. It helps us to avoid confusion, make things consistent and as result, when you are onboarding a new user, it takes them less time to learn things.\nAt high level, it may sound easy to create common API structure for all the models but it is not. Many models take input in form of np.ndarray, pd.Series, pd.DataFrame, xarray objects and some libraries implement their own data object. To handle this project my current go to approach is to parse the inner working of a model in a sklearn wrapper with common methods, signature and input and output. In this example, we will see how we can do this using cvxpy as optimizer and sklearn wrapper to generate a sklearn model object. Same thing can be done for statsmodels, pytorch, tensorflow or any other custom logic.\nTo start with we can create a conda virtual env (assuming conda is already installed. Otherwise install conda python 3.8 first and then revisit this.). Run the following block to create a virtual env in conda and install the required libraries,\nconda create -n sklearn_wrapper python=3.8 -y conda activate sklearn_wrapper pip install scikit-learn cvxpy jupyter notebook pandas Once this is done, we are good to start. Open a jupyter notebook and execute the following lines in a cell,\nfrom sklearn.base import BaseEstimator, RegressorMixin from sklearn.utils.validation import check_X_y, check_array, check_is_fitted import cvxpy as cp import numpy as np import pandas as pd Here we are importing BaseEstimator and RegressorMixin which our custom model class will inherit from. BaseEstimator will have default score method which can be over written and RegressorMixin will have get_params and set_params methods which can be useful later while execution to change to value of a model parameter. Other than these we are importing check_X_y, check_array and check_is_fitted from sklearn which will be used to check if the X and y we are passing is of sklearn convention or not, the variable we are passing is of array type of not and check_is_fitted is to prevent runningpredict before fit. If we run predict, any model will expect coeff or weight to make any prediction, until we run fit methods, these values will not be generated.\nclass CVXSkLearnWrapper(BaseEstimator, RegressorMixin): def __init__(self, alpha=1.0): self.alpha = alpha def _loss_fn(self, X, y, beta): return cp.norm2(X @ beta - y)**2 def _regularizer(self, beta): return cp.norm1(beta) def _obj_fn(self, X, y, beta, lambd): return self._loss_fn(X, y, beta) + lambd * self._regularizer(beta) def _mse(self, X, Y, beta): return (1.0 / X.shape[0]) * self._loss_fn(X, Y, beta).value def fit(self, X, y): X, y = check_X_y(X, y) n = X.shape[1] beta = cp.Variable(n) lambd = cp.Parameter(nonneg=True) problem = cp.Problem(cp.Minimize(self._obj_fn(X, y, beta, lambd))) lambd.value = self.alpha problem.solve() self.coeff_ = beta.value self.intercept_ = 0.0 return self def predict(self, X): check_is_fitted(self) X = check_array(X) return X @ self.coeff_ Here is the implementation LASSO using CVXPY as SKLearn model. Here in __init__ method we are taking alpha as user input. Note, all the argument in __init__ method should have a default values. While storing them in self the argument name of __init__ and reference in self should be same. For example if we are using __init__(self, alpha=1.0) then it must be store in self as self.alpha = alpha. This is a parameter which can be changed during execution using get_params and set_params method. To know about this optimization more check this link. Note, here inside fit method we have X, y = check_X_y(X, y) this is not ensure that the X, y variable we have passed to the fit method is correct with respect to data type and share. Also, fit method of sklearn always return self. In case of sklearn this is convention that any variable within in class with have a suffix _. Here also we are saving, coefficients in self.coeff_. Also, in predict we are using check_is_fitted(self) to check that the model has been fitted or not. I am using check_array(X) in fit to ensure that the argument X is of type array.\nIn the following block we are generating some synthetic data to fit the above model. Here beta_star is the true parameter. X and Y is the data on which we will fit the model and will try to estimate unknow beta_star with derived beta_hat.\ndef generate_data(m=100, n=20, sigma=5, density=0.2): \u0026quot;\u0026quot;\u0026quot;Generates data matrix X and observations Y.\u0026quot;\u0026quot;\u0026quot; np.random.seed(1) beta_star = np.random.randn(n) idxs = np.random.choice(range(n), int((1-density)*n), replace=False) for idx in idxs: beta_star[idx] = 0 X = np.random.randn(m,n) Y = X.dot(beta_star) + np.random.normal(0, sigma, size=m) return X, Y, beta_star In the following block we are generating the synthetic dataset, initializing lasso model with class CVXSkLearnWrapper and CVXSkLearnWrapper. After that we are executing fit which will generate the coeffs beta_hat, predict will generate prediction y_hat and score will calculate R-sq between y and y_hat.\nX, y, _ = generate_data() lasso = CVXSkLearnWrapper(alpha = 1.1) model = lasso.fit(X, y) model.predict(X) model.score(X, y) Notes:\nHere we are using RegressorMixin but depending on the model type it any can ClassifierMixin, RegressorMixin, ClusterMixin or TransformerMixin. All the methods and signature of them should be same if you are implementing more than one model to build a library. If you have _ prefix before any method in the model class, it will not be exposed. It will be considered as internal method and can be accessed with the class. If we inherit from BaseEstimator \u0026amp; RegressorMixin there will be a default score function but it can be overwritten. To change score function in hyper-parameter tuning you can use make_scorer and greater_is_better in it. Dynamic parsing of args is possible in __init__ method using inspect module, import inspect def __init__(self, arg1, arg2, arg3, ..., argN): args, _, _, values = inspect.getargvalues(inspect.currentframe()) values.pop(\u0026quot;self\u0026quot;) for arg, val in values.items(): setattr(self, arg, val) In this example, we are overwriting case score function using mean_absolute_percentage_error. Lets assume that for some reason we want to use MAPE instate as default scoring method it can be useful. Other using make_scorer can be used for any other custom score function or other sklearn score functions. from sklearn.base import BaseEstimator, RegressorMixin from sklearn.utils.validation import check_X_y, check_array, check_is_fitted import cvxpy as cp import numpy as np import pandas as pd from sklearn.metrics import mean_absolute_percentage_error class CVXSkLearnWrapper(BaseEstimator, RegressorMixin): def __init__(self, alpha=1.0): self.alpha = alpha def _loss_fn(self, X, y, beta): return cp.norm2(X @ beta - y)**2 def _regularizer(self, beta): return cp.norm1(beta) def _obj_fn(self, X, y, beta, lambd): return self._loss_fn(X, y, beta) + lambd * self._regularizer(beta) def _mse(self, X, Y, beta): return (1.0 / X.shape[0]) * self._loss_fn(X, Y, beta).value def fit(self, X, y): X, y = check_X_y(X, y) n = X.shape[1] beta = cp.Variable(n) lambd = cp.Parameter(nonneg=True) problem = cp.Problem(cp.Minimize(self._obj_fn(X, y, beta, lambd))) lambd.value = self.alpha problem.solve() self.coeff_ = beta.value self.intercept_ = 0.0 return self def predict(self, X): check_is_fitted(self) X = check_array(X) return X @ self.coeff_ def score(self, X, y): y_hat = self.predict(X) return mean_absolute_percentage_error(y, y_hat) All the hyper-parameters (not derived from data) has to be initialized in __init__ method. Any model parameters (derived from data) must be initialized in fit. Variable names in init should be always same as arg name and variables in fit should always have a suffix _. fit and predict are mandatory methods in BaseEstimator class. Reference:\nLASSO regression using CVXPY Creating your own estimator in scikit-learn Introduction to Scikit-Learn How to create/customize your own scorer function in scikit-learn? Metrics and scoring: quantifying the quality of predictions ","date":1671321600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671385700,"objectID":"0027140bdf2f1fecd8eef8b152e3b5f4","permalink":"https://academic-demo.netlify.app/post/lasso-using-scikit-learn-wrapper-and-cvxpy/","publishdate":"2022-12-18T00:00:00Z","relpermalink":"/post/lasso-using-scikit-learn-wrapper-and-cvxpy/","section":"post","summary":"Those who are working in ML space of sometime must be aware that there are multiple python libraries out there which support different algorithms/models. When you are trying to implement best model it is not possible to use just a single library or even a single language and compute infra. Also, note all the libraries are do not have the same method, signature and input and output. Hence if you are working on a library which is collection of multiple models from different library one good starting point can be to standardize you class, methods, signature and input and output. It helps us to avoid confusion, make things consistent and as result, when you are onboarding a new user, it takes them less time to learn things.\n","tags":["ML","python"],"title":"LASSO using Scikit-Learn wrapper \u0026 CVXPY","type":"post"},{"authors":null,"categories":null,"content":"I am working on RecSys to generate product recommendations for ABI\u0026rsquo;s B2B platform BEEs. Some of the challenges involved in the project include building AutoML for best hyper-parameter selection, distributed model training. feature store integration, building a python library for curated ML models with default configs, deployment of models in cloud native compute and many more. Super excited to work in this work stream with an amazing team.\nAlgorithm related challenges: Cross validation: How to perform cross validation for RecSys. How to link statistical metrics with business KPIs. Determining weighage between model goodness of fit and business KPIs. How to create a scoring function which can compare between different models during cross validation. Managing splitting strategy to ensure that models are comparable. Model selection: Single model or market-based model or hybrid model - combined of two or more models? Time/Sequence based models(LSTM/GRU)? Hyper parameter tuning: What can be the preferable hyper-parameter tuning framework, which can support GPU (Wide and Deep), Spark (ALS) and CPU (SAR etc.). KPIs: Evaluate existing KPIs such as Map@K, NDCG@K and improve if possible. Hybrid model or mixture of model: Also, what type of hybrid - sequential, parallel or weighted? As of now, two use case (conceptually) AutoML: Example of AutoML for multi-country setup (including hybrid model, hyper-parameter tuning) with recommended tech stack. Model drift, data drift, retraining and model monitoring: How to build a framework which can be integrated with the python library to detect model drift, data drift, retraining requirements and monitor generated results in online and offline models. Others: Backtesting, AB testing, linking online and offline evaluation. Programming \u0026amp; Infra related challenges: Code spaces: How a developer can use code space for CPU based workflow for day-to-day development. Managing multiple envs base of Spark/GPU/CPU dependencies using devcontainer. Can the same image be used in AML/ADB? AML + VS Code/Code Space Integration: Attaching AML compute to VS Code as terminal and jupyter kernel. Run experiments in AML without leaving VS Code/Code spaces. Triggering multiple concurrent jobs (not always same as distributed model training. Some of our models are classical models which we are running multiple times as embarrassingly parallel workload) in AML from VS Code which can scale in multiple nodes to run different models and return results in a fan out fan in pattern to a cloud storage. (One additional information here is we want to leverage all the cores within a node using joblib, hence the auto scaling we are expecting is at node level for a given threshold) mlflow integration with VS Code and AML. ADB + VS Code/Code Space Integration: Run experiment in ADB without leaving VS Code/Code spaces. Debugging: Using VS Code visual debugger in a distributed workflow in AML \u0026amp; ADB. Observability: Monitoring aggregated logs from different nodes in VS Code. Testing: How to run property based testing for ML models in distributed compute environments. Library: Managing multiple dependencies such as pyspark, GPU and CPU level system dependencies. Usage of JIT within and across models taking execution infra into account. Making library infra agnostic. If you are excited about solving above mentioned challenges feel free to reach out to me.\n","date":1671321600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671321600,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"https://academic-demo.netlify.app/project/internal-project/","publishdate":"2022-12-18T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"RecSys with AutoML.","tags":["RecSys","Python","ML","AutoML"],"title":"RecSys for B2B platform","type":"project"},{"authors":[],"categories":["programming"],"content":"Exploring poetry for dependency management in python In general pip \u0026amp; venv is a good combination of tool when you don\u0026rsquo;t have to manage multiple dependencies for your project. But imaging that in a project you need to management multiple dependency files to deploy code into multiple envs. It is possible to do this with pip, but in that case you need to manage multiple requirements files. To solve this project I have checked a few alternative like pyenv, pipx, pipenv, poetry etc. According to my experience, poetry is the simplest and most efficient one. I was checking some of the useful tutorials about this and here I am just taking a note of some of the useful points regarding this tool.\nSome useful poetry commands # Download poetry in Ubuntu curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python - source $HOME/.poetry/env # Add to PATH poetry --version # Check version of poetry poetry self update # Update version poetry new project1 # Create a new project cd project1 tree . poetry run pytest # Run pytest for the project poetry add pandas # Add a package as dependency of a project poetry remove pandas # Delete a project from the file poetry add --dev pytest # Add a package as dev dependency in a poetry project poetry add -D coverage[toml] pytest-cov # --dev \u0026amp; -D same poetry install # Install all the dependencies for a project poetry build # Build a python library using poetry poetry publish # Publish library to PyPI poetry export - requirements.txt --output requirements.txt # Generate requirements.txt poetry use python3.8 # Use specific version of python in the project Some important information Important files pyproject.toml is the single file for all project related metadata. poetry.lock file is the granular metadata. .pypirc will not work with poetry. config.toml \u0026amp; auth.toml is used for setting up the artifact repository. export POETRY_PYPI_TOKEN_PYPI, export POETRY_HTTP_BAISC_PYPI_USERNAME and export POETRY_HTTP_BAISC_PYPI_PASSWORD can be used for this. Publishing library as artifact to artifact store # config.toml : ~/.config/pypoetry/config.toml [repositories] pypi = {url = \u0026quot;https://upload.pypi.org/legacy/\u0026quot;} testpypi = {url = \u0026quot;https://test.pypi.org/legacy/\u0026quot;} # auth.toml: ~/.config/pypoetry/auth.toml [http-basic] pypi = {username = \u0026quot;myuser\u0026quot;, password = \u0026quot;topsecret\u0026quot;} testpypi = {username = \u0026quot;myuser\u0026quot;, password = \u0026quot;topsecret\u0026quot;} Check GitHub issue related to this here.\nReference: PyBites Python Poetry Training ","date":1639785600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639815162,"objectID":"eb37619b62b0171d9a6b99a20a7d4af3","permalink":"https://academic-demo.netlify.app/post/avoid-relative-path-import-hell-in-python/","publishdate":"2021-12-18T00:00:00Z","relpermalink":"/post/avoid-relative-path-import-hell-in-python/","section":"post","summary":"Exploring poetry for dependency management in python In general pip \u0026amp; venv is a good combination of tool when you don\u0026rsquo;t have to manage multiple dependencies for your project. But imaging that in a project you need to management multiple dependency files to deploy code into multiple envs. It is possible to do this with pip, but in that case you need to manage multiple requirements files. To solve this project I have checked a few alternative like pyenv, pipx, pipenv, poetry etc. According to my experience, poetry is the simplest and most efficient one. I was checking some of the useful tutorials about this and here I am just taking a note of some of the useful points regarding this tool.\n","tags":["python"],"title":"Avoid relative path import hell in python","type":"post"},{"authors":[],"categories":[],"content":"When we are working in an collaborative work environment, this is important to share a common set of convention which makes collaboration easier. Here are some of the common gitops best practice which has helped to collaborate better,\nRepository naming convention {short product name}_{component}_{type of repository}\nShort product name: modeller: modelling engine optimizer: optimization engine Component: ui: any code related to the user interface ml: anything to do with machine learning code base de: anything to do with data engineering code base be: anything to do with REST API endpoints for ml, de or in general application. Type of repository: app: mostly used for ui and cli standalone application. lib: library code base of ml and de engines. job: cli or script based jobs which will be executed in a batch process. api: code base related to apis. Example: modeller_ml_lib modeller_de_lib modeller_ml_job modeller_be_api modeller_ui_app Branch naming convention: Code Flow Branches (restricted commit branch): develop: developers can merge their branches here. staging: any final tagging or testing has to happen here. master: production branch, if all the validation is working in staging and tested code needs to be merged here. test: other than unit testing regression, integration and end to end testing has to happen here.\nFlow of code: develop \u0026gt; test \u0026gt; staging \u0026gt; production\nTemporary Branches: feature: anything which is a feature has to be developed in a feature branch and the branch name should be like : feature/{name of the feature} [example: feature/integrate-swagger] bug fix: any bug fix has to be done in a bud fix branch. the structure remains as feature branch. structure of the branch name will be like bugfix/{bug which is being fixed} [example: bugfix/more-gray-shades-in-loader] hot fix: any hotfix from master has to be made in this branch. also, note that these fixes has to be merged in other common branches that the master branch. [example: hotfix/disable-endpoint-zero-day-exploit] experimental: any experimental work has to be tagged this way while creating a branch. [example: experimental/dark-theme-support] build: any build branch should start with the build pre-fix. [example: build/jacoco-metric] release: any release branch has to be tagged by a release prefix. [example: release/myapp-1.01.123] merging: any intermediate merge branch has to be tagged with merge prefix. [example: merge/dev_lombok-refactoring] Commit convention: We are adding some keyword to the commit messages based on which a semantic versioning tag can be generated. A version tag will look like, v{MAJOR}.{MINOR}.{PATCH}\nHere, this is how commit tags are getting translated to semantic versioning numbers, fix --\u0026gt; patch, feat --\u0026gt; minor and BREAKING CHANGE --\u0026gt; major.\nArea of work: fix: if there is a small change which does not break any of the high level APIs and does not change overall behaviors of a feature that will be consider as fix. this corresponds to the patch in the semantic versioning. feat: if there is a feature level change then the commit should have feat tagged along with in. in terms of the semantic versioning this corresponds to the minor version change. BREAKING CHANGE: this is a change where multiple modules and high level APIs are getting changed. build: These are not linked with semver. any build level changes has to be tagged as build. for example, if we are changing some configuration in setup.py that should be tagged as build commit. chore: updating grunt tasks etc; no production code change ci: change in any of the github workflow or azure pipeline level changes has to be tagged as ci commit. docs: any changes in wiki documentation or library docstring level changes has to be tagged as docs. style: if there is any changes in code due to formatting or linting those can be tagged as style change. refactor: if some code is being refactored that has to be tagged as refactored commit. perf: for profiling a code base use pref tag. test: for any commit related to tests can be tagged as test. Commit structure: {area_of_work}: {ticket_id} {One line description of commit} [optional] Detailed description of the commit In general things related to SCM: Do not push large files in git. Do not pass secrets in git. Do not directly commit in no commit branches. Generate .gitignore file from gitignore.io. Reference: Conventional Commits Pattern Python library version management with GitHub action + commitizen + pre-commit hook Conventional commits ","date":1635638400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635679988,"objectID":"ee95c0a8d24b359fb9d12755c1fd609c","permalink":"https://academic-demo.netlify.app/post/scm-conventions-ways-of-working/","publishdate":"2021-10-31T00:00:00Z","relpermalink":"/post/scm-conventions-ways-of-working/","section":"post","summary":"When we are working in an collaborative work environment, this is important to share a common set of convention which makes collaboration easier. Here are some of the common gitops best practice which has helped to collaborate better,\n","tags":[],"title":"SCM conventions \u0026 ways of working","type":"post"},{"authors":[],"categories":[],"content":"In this post I\u0026rsquo;m going to discuss three docker tools which I find really useful in my day to day workflow. These tools are,\nDive Portainer dozzle Dive: quick check of docker images Dive is super useful to check if there any unnecessary data stored in a docker image. In Ubuntu to install dive run the following commands,\nwget https://github.com/wagoodman/dive/releases/download/v0.9.2/dive_0.9.2_linux_amd64.deb sudo apt install ./dive_0.9.2_linux_amd64.deb Note, replace this 0.9.2 version to the latest one. Once this tool is installed in your system you can use,\ndive \u0026lt;your-image-tag\u0026gt; or\ndive build -t \u0026lt;some-tag\u0026gt; . You can use dive as GitHub action in your workflow. Checkout this action which helps integrating dive in your workflow using github action.\nDozzle: a light-weight centralized log monitoring tool for containers This an useful tool which can be used for monitoring all the running containers live logs. This can be used with normal docker-compose or docker swarm.\ndozzle: container_name: dozzle image: amir20/dozzle:latest volumes: - /var/run/docker.sock:/var/run/docker.sock ports: - \u0026quot;8001:8080\u0026quot; If you add the above section in any of the existing docker-compose file this will work out of the box and will show all the running logs of the same docker compose file in localhost:8001.\nPortainer: GUI to monitor all running container This tool is good to see all the all docker containers in the system and meta data related to the docker containers.\nPortainer 2.0 out of the box works with,\ndocker docker-compose docker-swarm kubernetes azure container registry Reference: https://www.youtube.com/watch?v=QBNaOdNSsx8 https://github.com/amir20/dozzle https://docs.portainer.io/ ","date":1635638400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635668578,"objectID":"f19224b4d69cea17b2fd37b16e7fdc7e","permalink":"https://academic-demo.netlify.app/post/useful-tools-for-docker-container-management/","publishdate":"2021-10-31T00:00:00Z","relpermalink":"/post/useful-tools-for-docker-container-management/","section":"post","summary":"In this post I\u0026rsquo;m going to discuss three docker tools which I find really useful in my day to day workflow. These tools are,\nDive Portainer dozzle Dive: quick check of docker images ","tags":[],"title":"Useful tools for docker container management","type":"post"},{"authors":[],"categories":[],"content":"List of tools used in this post commitizen conventionalcommits Git Hooks Pre-commit hook GitHub Action Semantic Versioning Here is a sample repository which show the implementation below.\nContext \u0026amp; overview Lets assume that we are going to build a python library. Lets call it minipackage. We need to manage the version of the library \u0026amp; need to generate the CHANGELOGS.md whenever we release a version of the library. There are multiple ways to solve this problem. Here in the post we are going see how we can use semantic versioning to solve this problem.\nProject directory structure minipackage: All the source code for the python library will live here. __init.py__: Library version, other meta data will be here. Also, we need to register any external methods we want to expose as API in the library. main.py: main module is kind of representation of any module and sub-module which will be part of the python library. pyproject.toml: This file will have all the meta-data related to the tools which we will be using this project. In the example, tool.commitizen example is present but there can be many more settings registered in this file related to the other tools being used in the project. README.md: self explanatory. requirements.txt: All the python dependencies required for the library. .pre-commit-config.yaml: git pre-commit hooks. Here in this example we have just shown the commitizen pre-commit hook example. But there can be many other as well. setup.py: all the library building related information will be here. If you want to refresh some basics of python library building process checkout this link. Regarding pre-commit checkout this link.\n. â”œâ”€â”€ minipackage â”‚Â â”œâ”€â”€ __init__.py â”‚Â â”œâ”€â”€ main.py â”œâ”€â”€ pyproject.toml â”œâ”€â”€ README.md â”œâ”€â”€ requirements.txt â”œâ”€â”€ .pre-commit-config.yaml â””â”€â”€ setup.py pyproject.toml: project tools and metadata related to tools used in project version and version_files these are the two import field which has a major impact in this workflow. make sure that version_files field points to the file which is the single source of version. We also need to ensure that we will be using the same file and variable all across the project. While start a project __version__ in __init__.py and version variable in pyproject.toml file should match.\n[tool.commitizen] name = \u0026quot;cz_conventional_commits\u0026quot; version = \u0026quot;0.0.1\u0026quot; version_files = [ \u0026quot;minipackage/__init__.py\u0026quot;, \u0026quot;pyproject.toml:version\u0026quot; ] tag_format = \u0026quot;v$version\u0026quot; bump_message = \u0026quot;bump: $current_version â†’ $new_version [skip-ci]\u0026quot; Note, here in our example here we using the variable __version__ in all place in the project to manage the version and the same file is mentioned in the pyproject file. Now, when we using commitizen to bump version of the library it will change value of this variable depending on the commit message tags. Also, this will generate a changelog.md file in the project root directory and create a release tag.\nsetup.py: library build related information In this section, notice that we using __version__ variable from minipackage module here. We are passing this __version__ variable to in version argument in setup function. This will ensure whenever we are building a library, the library gets tagged with this version.\nNote, that whatever is being defined or imported in minipackage/__init__py file will be present in in minipackage module.\nfrom setuptools import find_packages, setup from minipackage import __version__ setup( author=\u0026quot;Aritra Biswas\u0026quot;, author_email=\u0026quot;pandalearnstocode@gmail.com\u0026quot;, python_requires=\u0026quot;\u0026gt;=3.8\u0026quot;, install_requires=requirements, include_package_data=True, keywords=\u0026quot;minipackage\u0026quot;, name=\u0026quot;minipackage\u0026quot;, packages=find_packages(include=[\u0026quot;minipackage\u0026quot;, \u0026quot;minipackage.*\u0026quot;]), version=__version__, ) source/__init__.py: main place where the version variable is being used This will be the initial state of the init file. Later when we bump library version in the CI pipeline, this __version__ variable will change depending upon commit message tags.\n__version__ = \u0026quot;0.0.1\u0026quot; from minipackage.main import hello_world, hello_mcu, hello_dc, .pre-commit-config.yaml : ensuring commit message format is being followed This is not a mandatory thing but kind of a fail safe mechanism to implement conventional commit messages in our day to day workflow.\nrepos: - repo: https://github.com/commitizen-tools/commitizen rev: v2.19.0 hooks: - id: commitizen stages: [commit-msg] One this pre-commit hook is install to a repository, whenever we are going to make commit this will check the commit tags are present in the commit message or not. To know more about this in depth go through this site.\n.github/workflows/bumpversion.yaml : GitHub action to update the version value in the respective file For a commit message like bump: update library version. to the configured branch, this will update the library version, generate changelog and push it to the feature branch. Post that when we trigger a build, a library with the same version tag will be generated and sent to the python library repository.\nname: NEW Bump version on: push: branches: - develop jobs: bump_version: if: \u0026quot;!startsWith(github.event.head_commit.message, 'bump:')\u0026quot; runs-on: ubuntu-latest name: \u0026quot;Bump version and create changelog with commitizen\u0026quot; steps: - name: Check out uses: actions/checkout@v2 with: fetch-depth: 0 token: \u0026quot;${{ secrets.GITHUB_TOKEN }}\u0026quot; - id: cz name: Create bump and changelog uses: commitizen-tools/commitizen-action@master with: github_token: ${{ secrets.GITHUB_TOKEN }} branch: develop - name: Print Version run: echo \u0026quot;Bumped to version ${{ steps.cz.outputs.version }}\u0026quot; Note: This workflow may change depending upon how you want to update the library version. Here action is driven by push to develop branch but it is possible to setup this process with pull request trigger or any other trigger as well.\nReference: Conventional Commits Pattern ","date":1635033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635054752,"objectID":"8186b90611bfead9ea5839641e510fed","permalink":"https://academic-demo.netlify.app/post/manage-python-library-versioning-using-commitizen-pre-commit-hook/","publishdate":"2021-10-24T00:00:00Z","relpermalink":"/post/manage-python-library-versioning-using-commitizen-pre-commit-hook/","section":"post","summary":"List of tools used in this post commitizen conventionalcommits Git Hooks Pre-commit hook GitHub Action Semantic Versioning Here is a sample repository which show the implementation below.\nContext \u0026amp; overview Lets assume that we are going to build a python library. Lets call it minipackage. We need to manage the version of the library \u0026amp; need to generate the CHANGELOGS.md whenever we release a version of the library. There are multiple ways to solve this problem. Here in the post we are going see how we can use semantic versioning to solve this problem.\n","tags":[],"title":"Python library version management with GitHub action + commitizen + pre-commit hook","type":"post"},{"authors":[],"categories":[],"content":"This is important to have a working local development environment for quick prototyping and developing CRUD application using Fast API. Here in this post we are going to explore how we can quickly spin up and a local development environment. Here we are going to use the following tools to develop the REST API endpoints,\nFast API SQLModel SQLite Alembic Prerequisite: Anaconda DB browser for SQLite curl Code repository: Fast API + SQLite + SQLModel + Alembic (Sync version) Fast API + SQLite + SQLModel + Alembic (Async version) Sample API workflow: Workflow: Write database models Write database settings Write REST API endpoints Test APIs Setup alembic Change database models Update APIs Run alembic migration Test APIs Project structure: . â”œâ”€â”€ app â”‚ â”œâ”€â”€ db.py : All the settings related to DB will be here. â”‚ â”œâ”€â”€ __init__.py â”‚ â”œâ”€â”€ main.py : All endpoints will be defined here. â”‚ â””â”€â”€ models.py : All the data models will be defined here. â””â”€â”€ database.db : This SQLite DB will be created and data will be stored here. Step 1: Setup development environment Create a local development conda environment to run the application from local. Here we are going to use python 3.8. After creating the environment install the required dependencies.\nconda create --name fastapi_sqlmodel python=3.8 conda activate fastapi_sqlmodel pip install fastapi[all] sqlmodel alembic aiosqlite Step 2: Create database models in models.py Here we are going to define a database model to store information related to task name. Task table is the table which will be updated later with the task description information and we will perform a migration to see how the changes are going to be maintained and reflected in the database.\nfrom sqlmodel import SQLModel, Field class TaskBase(SQLModel): task_name: str class Task(TaskBase, table=True): id: int = Field(default=None, primary_key=True) class TaskCreate(TaskBase): pass Step 3: Create settings related to DB in db.py Here for rapid prototype we are going to use sqlite local db. Which is kind of a standalone file. To view the record in the file we can we use the DB browser for SQLite. After we run the application a a file called database.db will be created in the root directory of the project.\nSync version: Here are going to create two functions which will create all the required tables when the FastAPI application starts and will generate a session using which I/O operations will be performed in a DB.\nfrom sqlmodel import Session, SQLModel, create_engine sqlite_file_name = \u0026quot;database.db\u0026quot; sqlite_url = f\u0026quot;sqlite:///{sqlite_file_name}\u0026quot; connect_args = {\u0026quot;check_same_thread\u0026quot;: False} engine = create_engine(sqlite_url, echo=True, connect_args=connect_args) def create_db_and_tables(): SQLModel.metadata.create_all(engine) def get_session(): with Session(engine) as session: yield session Async version: Asyn version of the above defined functions which will be used in main.py.\nfrom sqlmodel import SQLModel from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine from sqlalchemy.orm import sessionmaker sqlite_file_name = \u0026quot;database.db\u0026quot; sqlite_url = f\u0026quot;sqlite+aiosqlite:///{sqlite_file_name}\u0026quot; engine = create_async_engine(sqlite_url, echo=True, future=True) async def init_db(): async with engine.begin() as conn: await conn.run_sync(SQLModel.metadata.create_all) async def get_session() -\u0026gt; AsyncSession: async_session = sessionmaker( engine, class_=AsyncSession, expire_on_commit=False ) async with async_session() as session: yield session Step 4: Define API endpoints in main.py Here we are going to define a POST endpoint using which we can write some task related information in the database. As of now, the request model for this endpoint is TaskCreate and in the response this endpoint will return an object of type Task.\nSync version: from fastapi import FastAPI, Depends from sqlmodel import Session from app.db import create_db_and_tables, get_session from app.models import Task, TaskCreate app = FastAPI() @app.on_event(\u0026quot;startup\u0026quot;) def on_startup(): create_db_and_tables() @app.get(\u0026quot;/ping\u0026quot;) def pong(): return {\u0026quot;ping\u0026quot;: \u0026quot;pong!\u0026quot;} @app.post(\u0026quot;/task/\u0026quot;, response_model=Task) def create_task(task: TaskCreate, session: Session = Depends(get_session)): db_task = Task.from_orm(task) session.add(db_task) session.commit() session.refresh(db_task) return db_task Async version: from fastapi import FastAPI, Depends from sqlmodel import Session from app.db import create_db_and_tables, get_session from app.models import Task, TaskCreate app = FastAPI() @app.on_event(\u0026quot;startup\u0026quot;) def on_startup(): create_db_and_tables() @app.get(\u0026quot;/ping\u0026quot;) def pong(): return {\u0026quot;ping\u0026quot;: \u0026quot;pong!\u0026quot;} @app.post(\u0026quot;/task/\u0026quot;, response_model=Task) def create_task(task: TaskCreate, session: Session = Depends(get_session)): db_task = Task.from_orm(task) session.add(db_task) session.commit() session.refresh(db_task) return db_task Step 5: Test API endpoints Here we are going to test the API endpoints are working fine or not. If we get the expected result from the CURL and can validate the records are being updated in the DB we can expect that the APIs are working fine.\nuvicorn app.main:app --reload curl -X POST http://127.0.0.1:8000/task/ -H 'accept: application/json' -H 'Content-Type: application/json' -d '{\u0026quot;task_name\u0026quot;: \u0026quot;just added task\u0026quot;}' Note:\na database.db file will be create in project root directory. after making the post call validate the db records are being updated using DB browser for SQLite Step 6: Setup alembic Step 6a: Generate alembic settings In this step we are going generate all the settings for alembic migration. This will generate some files and folders in the project root directory.\nSync version: alembic init alembic Async version: alembic init -t async alembic Step 6b: Update alembic settings After these files are generated, change the DB url in alembic.ini.\nSync version: replace sqlalchemy.url = driver://user:pass@localhost/dbname in alembic.ini with sqlite:///database.db. Async version: replace sqlalchemy.url = driver://user:pass@localhost/dbname in alembic.ini with sqlite+aiosqlite:///database.db. Common changes for Sync \u0026amp; Async version: in alembic/env.py file add from sqlmodel import SQLModel in the import section. in alembic/env.py file add the following line in import section, from app.models import Task. in alembic/env.py change target_metadata = None to target_metadata = SQLModel.metadata. in alembic/script.py.mako add import sqlmodel in the import section. Step 6c: Generate alembic migration settings After making all the changes, make first migration.\nalembic revision --autogenerate -m \u0026quot;init\u0026quot; alembic upgrade head Step 7: Update data models in models.py Now, lets say there is some changes we need to add to our data model. In this case we are just going to add task_description. task_description will be a optional string column in the Task table. After we make the required changes in the app/models.py and app/main.py, we need to run the migration to add the newly added columns in the DB.\nfrom sqlmodel import SQLModel, Field from typing import Optional # This is a new add line class TaskBase(SQLModel): task_name: str task_description: Optional[str] = None # This is a new add line class Task(TaskBase, table=True): id: int = Field(default=None, primary_key=True) class TaskCreate(TaskBase): pass Step 8: Update API endpoints in main.py In this case, endpoints remains unchanged but in a realistic scenario endpoints has to be changed to take the new information into account.\nStep 9: Run DB migration Run the following line to update the database tables.\nalembic revision --autogenerate -m \u0026quot;add description\u0026quot; alembic upgrade head Step 10: Test updated API endpoints As last step, make a call to the API endpoints which will be using the updated table and check if everything is working fine or not.\nuvicorn app.main:app --reload curl -X POST http://127.0.0.1:8000/task/ -H 'accept: application/json' -H 'Content-Type: application/json' -d '{\u0026quot;task_name\u0026quot;: \u0026quot;just added task\u0026quot;,\u0026quot;task_description\u0026quot;:\u0026quot;a newly created task\u0026quot;}' after making the post call validate the db records are being updated using DB browser for SQLite Conclusion: This workflow is quite good when in come to rapid prototyping. One can use this inside a docker container as well but make sure that the volume mounting is in place to keep the standalone database file and the other related alembic migration scripts.\nReference: FastAPI with Async SQLAlchemy, SQLModel, and Alembic Async SQL (Relational) Databases FastAPI + SQLModel + Alembic SQL (Relational) Databases Building a Phone Directory with Python, MySQL, FastAPI, and Angular Alembic: Auto Generating Migrations Build an async python service with FastAPI \u0026amp; SQLAlchemy How to set up FastAPI, Ormar, and Alembic tiangolo/uvicorn-gunicorn-fastapi-docker ","date":1634342400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634348073,"objectID":"e81c776cd8e2fb3a98f0abd907f6811f","permalink":"https://academic-demo.netlify.app/post/local-development-environment-with-fast-api-sqlmodel-sqlite-alembic-sync-async-version/","publishdate":"2021-10-16T00:00:00Z","relpermalink":"/post/local-development-environment-with-fast-api-sqlmodel-sqlite-alembic-sync-async-version/","section":"post","summary":"This is important to have a working local development environment for quick prototyping and developing CRUD application using Fast API. Here in this post we are going to explore how we can quickly spin up and a local development environment. Here we are going to use the following tools to develop the REST API endpoints,\n","tags":[],"title":"Local development environment with Fast API + SQLModel + SQLite + Alembic","type":"post"},{"authors":["Aritra Biswas"],"categories":null,"content":" Create your slides in Markdown - click the Slides button to check out the example. Supplementary notes can be added here, including code, math, and images.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://academic-demo.netlify.app/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne **Two** Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}} Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://academic-demo.netlify.app/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://academic-demo.netlify.app/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":["Aritra Biswas","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. Create your slides in Markdown - click the Slides button to check out the example. Supplementary notes can be added here, including code, math, and images.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://academic-demo.netlify.app/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Aritra Biswas","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. Create your slides in Markdown - click the Slides button to check out the example. Supplementary notes can be added here, including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"https://academic-demo.netlify.app/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://academic-demo.netlify.app/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]