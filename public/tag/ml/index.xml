<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ML | Aritra Biswas</title>
    <link>https://academic-demo.netlify.app/tag/ml/</link>
      <atom:link href="https://academic-demo.netlify.app/tag/ml/index.xml" rel="self" type="application/rss+xml" />
    <description>ML</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 20 Dec 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://academic-demo.netlify.app/media/icon_hu966111359855668688.png</url>
      <title>ML</title>
      <link>https://academic-demo.netlify.app/tag/ml/</link>
    </image>
    
    <item>
      <title>Exponential Smoothing using Scikit-Learn wrapper &amp; statsmodels</title>
      <link>https://academic-demo.netlify.app/post/exponential-smoothing-using-scikit-learn-wrapper-statsmodels/</link>
      <pubDate>Tue, 20 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/exponential-smoothing-using-scikit-learn-wrapper-statsmodels/</guid>
      <description>&lt;p&gt;As we discussed in previous post, in this series we will be working on different type of regression problem and will try to parse them as sklearn model objects. Here are we will be working with &lt;code&gt;ExponentialSmoothing&lt;/code&gt; from &lt;code&gt;statsmodels&lt;/code&gt; library. This is one of the most common time series model used in general. &lt;code&gt;statsmodels&lt;/code&gt; is heavily influence by R and its convention. If you take a deep dive into it&amp;rsquo;s functionality of or formula based approach in case of GLM, it is quite evident including the summary function.&lt;/p&gt;
&lt;p&gt;Here in this post our objective will be to take this &lt;code&gt;ExponentialSmoothing&lt;/code&gt; from &lt;code&gt;statsmodels&lt;/code&gt; and build a model class which looks like sklearn model and the method signatures are same in nature so there is less learning curve involved when there is a context switch or in this case a user is trying to build different models.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from statsmodels.tsa.holtwinters import ExponentialSmoothing
from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.utils.validation import check_X_y, check_array, check_is_fitted
from sklearn.metrics import mean_absolute_percentage_error
import numpy as np
import pandas as pd
import inspect

class HWTimeSeriesSkLearnWrapper(BaseEstimator, RegressorMixin):
    def __init__(
        self,
        trend=None,
        damped_trend=False,
        seasonal=None,
        seasonal_periods=None,
        use_boxcox=False,
        optimized=True,
        remove_bias=False,
        initialization_method = None,
        model=ExponentialSmoothing,
    ):
        args, _, _, values = inspect.getargvalues(inspect.currentframe())
        values.pop(&amp;quot;self&amp;quot;)
        for arg, val in values.items():
            setattr(self, arg, val)

    def fit(self, X, y = None):
        if not X.empty:
            self.X = X
        self.model_ = self.model(
            self.X,
            trend=self.trend,
            seasonal=self.seasonal,
            seasonal_periods=self.seasonal_periods,
            damped_trend=self.damped_trend,
            use_boxcox=self.use_boxcox,
        )
        self.result_ = self.model_.fit(
            optimized=self.optimized, remove_bias=self.remove_bias
        )
        self.level = self.result_.level
        self.trend = self.result_.trend
        self.season = self.result_.season
        self.fitted_values = self.result_.fittedvalues
        return self

    def predict(self, X, y = None):
        check_is_fitted(self, &amp;quot;result_&amp;quot;)
        self.df = self.result_.forecast(X.index.size).to_frame()
        self.df.columns = [f&amp;quot;predicted_{name}&amp;quot; for name in X.columns]
        self.df.index.name = X.index.name
        return self.df

    def score(self, X, y):
        y_true = X.to_numpy().flatten()
        y_hat = self.predict(X).to_numpy().flatten()
        return mean_absolute_percentage_error(y_true= y_true, y_pred=y_hat)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see in the above block, I have come up with this structure which can take all the args of &lt;code&gt;__init__&lt;/code&gt; and &lt;code&gt;fit&lt;/code&gt; of &lt;code&gt;ExponentialSmoothing&lt;/code&gt; while initializing the wrapper class &lt;code&gt;HWTimeSeriesSkLearnWrapper&lt;/code&gt;. Note, these are not all the exhaustive set of parameter. You can take a look into the source code of the model class from &lt;code&gt;statsmodels&lt;/code&gt; and figure out what are the other parameters which can be helpful for your context.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;args, _, _, values = inspect.getargvalues(inspect.currentframe())
values.pop(&amp;quot;self&amp;quot;)
for arg, val in values.items():
    setattr(self, arg, val)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above chunk of the code will help to initialize arbitrary number of parameters for your model and in subsequent methods such as initialization or fit you can pass the parameter values from self. In the next block, I am downloading the data and creating a train test split. Note, in case of time series we need to remember two things,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Train test split can not be random&lt;/li&gt;
&lt;li&gt;There is no such concept of train and test data. Index is kind of your core feature along with meta features which can be derived from target.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The catch here is we need to create train and test in a way it can look like sklearn model but under the hood it should be able to use the same datasets for building times series model using statsmodels or any other library you may use.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def get_and_split_airline_data(test_size = 0.2, date_col = &amp;quot;month&amp;quot;, date_format = &amp;quot;%Y-%m&amp;quot;, date_freq = &amp;quot;MS&amp;quot;):
    df = pd.read_csv(&amp;quot;https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv&amp;quot;)
    df.columns = df.columns.str.lower()
    if date_col in df.columns:
        df[date_col] = pd.to_datetime(df[date_col], format = date_format)
        df = df.sort_values(date_col).set_index(date_col)
        df.index.freq = date_freq
    train_size = int(df.shape[0] * (1 - test_size))
    X_train = df.iloc[:train_size]
    X_test = df.iloc[train_size:]
    y_train = X_train.copy()
    y_test = X_test.copy()
    return X_train,X_test,y_train, y_test
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, once we have the data we can use &lt;code&gt;fit&lt;/code&gt;, &lt;code&gt;predict&lt;/code&gt; and &lt;code&gt;score&lt;/code&gt; to calculate the required values.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_train,X_test,y_train, y_test = get_and_split_airline_data()
model = HWTimeSeriesSkLearnWrapper(trend = &amp;quot;add&amp;quot;, damped_trend = True, seasonal = &amp;quot;add&amp;quot;, seasonal_periods = 12)
model.fit(X_train, y_train)
model.predict(X_test)
model.score(X_test, y_test)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>LASSO using Scikit-Learn wrapper &amp; CVXPY</title>
      <link>https://academic-demo.netlify.app/post/lasso-using-scikit-learn-wrapper-and-cvxpy/</link>
      <pubDate>Sun, 18 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/lasso-using-scikit-learn-wrapper-and-cvxpy/</guid>
      <description>&lt;p&gt;Those who are working in ML space of sometime must be aware that there are multiple python libraries out there which support different algorithms/models. When you are trying to implement best model it is not possible to use just a single library or even a single language and compute infra. Also, note all the libraries are do not have the same method, signature and input and output. Hence if you are working on a library which is collection of multiple models from different library one good starting point can be to standardize you class, methods, signature and input and output. It helps us to avoid confusion, make things consistent and as result, when you are onboarding a new user, it takes them less time to learn things.&lt;/p&gt;
&lt;p&gt;At high level, it may sound easy to create common API structure for all the models but it is not. Many models take input in form of &lt;code&gt;np.ndarray&lt;/code&gt;, &lt;code&gt;pd.Series&lt;/code&gt;, &lt;code&gt;pd.DataFrame&lt;/code&gt;, &lt;code&gt;xarray&lt;/code&gt; objects and some libraries implement their own data object. To handle this project my current go to approach is to parse the inner working of a model in a sklearn wrapper with common methods, signature and input and output. In this example, we will see how we can do this using &lt;code&gt;cvxpy&lt;/code&gt; as optimizer and &lt;code&gt;sklearn&lt;/code&gt; wrapper to generate a &lt;code&gt;sklearn&lt;/code&gt; model object. Same thing can be done for &lt;code&gt;statsmodels&lt;/code&gt;, &lt;code&gt;pytorch&lt;/code&gt;, &lt;code&gt;tensorflow&lt;/code&gt; or any other custom logic.&lt;/p&gt;
&lt;p&gt;To start with we can create a &lt;code&gt;conda&lt;/code&gt; virtual env (assuming &lt;code&gt;conda&lt;/code&gt; is already installed. Otherwise install &lt;code&gt;conda&lt;/code&gt; python 3.8 first and then revisit this.). Run the following block to create a virtual env in &lt;code&gt;conda&lt;/code&gt; and install the required libraries,&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n sklearn_wrapper python=3.8 -y
conda activate sklearn_wrapper
pip install scikit-learn cvxpy jupyter notebook pandas
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once this is done, we are good to start. Open a &lt;code&gt;jupyter notebook&lt;/code&gt; and execute the following lines in a cell,&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.utils.validation import check_X_y, check_array, check_is_fitted
import cvxpy as cp
import numpy as np
import pandas as pd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we are importing &lt;code&gt;BaseEstimator&lt;/code&gt; and &lt;code&gt;RegressorMixin&lt;/code&gt; which our custom model class will inherit from. &lt;code&gt;BaseEstimator&lt;/code&gt; will have default score method which can be over written and &lt;code&gt;RegressorMixin&lt;/code&gt; will have &lt;code&gt;get_params&lt;/code&gt; and &lt;code&gt;set_params&lt;/code&gt; methods which can be useful later while execution to change to value of a model parameter. Other than these we are importing &lt;code&gt;check_X_y&lt;/code&gt;, &lt;code&gt;check_array&lt;/code&gt; and &lt;code&gt;check_is_fitted&lt;/code&gt; from &lt;code&gt;sklearn&lt;/code&gt; which will be used to check if the X and y we are passing is of sklearn convention or not, the variable we are passing is of &lt;code&gt;array&lt;/code&gt; type of not and &lt;code&gt;check_is_fitted&lt;/code&gt; is to prevent running&lt;code&gt;predict&lt;/code&gt; before &lt;code&gt;fit&lt;/code&gt;. If we run predict, any model will expect &lt;code&gt;coeff&lt;/code&gt; or &lt;code&gt;weight&lt;/code&gt; to make any prediction, until we run &lt;code&gt;fit&lt;/code&gt; methods, these values will not be generated.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class CVXSkLearnWrapper(BaseEstimator, RegressorMixin):
    def __init__(self, alpha=1.0):
        self.alpha = alpha
        
    def _loss_fn(self, X, y, beta):
        return cp.norm2(X @ beta - y)**2

    def _regularizer(self, beta):
        return cp.norm1(beta)

    def _obj_fn(self, X, y, beta, lambd):
        return self._loss_fn(X, y, beta) + lambd * self._regularizer(beta)

    def _mse(self, X, Y, beta):
        return (1.0 / X.shape[0]) * self._loss_fn(X, Y, beta).value

    def fit(self, X, y):
        X, y = check_X_y(X, y)
        n = X.shape[1]
        beta = cp.Variable(n)
        lambd = cp.Parameter(nonneg=True)
        problem = cp.Problem(cp.Minimize(self._obj_fn(X, y, beta, lambd)))
        lambd.value = self.alpha
        problem.solve()
        self.coeff_ = beta.value
        self.intercept_ = 0.0
        return self
    
    def predict(self, X):
        check_is_fitted(self)
        X = check_array(X)
        return X @ self.coeff_
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the implementation LASSO using &lt;code&gt;CVXPY&lt;/code&gt; as &lt;code&gt;SKLearn&lt;/code&gt; model. Here in &lt;code&gt;__init__&lt;/code&gt; method we are taking &lt;code&gt;alpha&lt;/code&gt; as user input. Note, all the argument in &lt;code&gt;__init__&lt;/code&gt; method should have a default values. While storing them in &lt;code&gt;self&lt;/code&gt; the argument name of &lt;code&gt;__init__&lt;/code&gt; and reference in &lt;code&gt;self&lt;/code&gt; should be same. For example if we are using &lt;code&gt;__init__(self, alpha=1.0)&lt;/code&gt; then it must be store in &lt;code&gt;self&lt;/code&gt; as &lt;code&gt;self.alpha = alpha&lt;/code&gt;. This is a parameter which can be changed during execution using &lt;code&gt;get_params&lt;/code&gt; and &lt;code&gt;set_params&lt;/code&gt; method. To know about this optimization more check &lt;a href=&#34;https://www.cvxpy.org/examples/machine_learning/lasso_regression.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this&lt;/a&gt; link. Note, here inside &lt;code&gt;fit&lt;/code&gt; method we have &lt;code&gt;X, y = check_X_y(X, y)&lt;/code&gt; this is not ensure that the &lt;code&gt;X, y&lt;/code&gt; variable we have passed to the &lt;code&gt;fit&lt;/code&gt; method is correct with respect to data type and share. Also, &lt;code&gt;fit&lt;/code&gt; method of &lt;code&gt;sklearn&lt;/code&gt; always return &lt;code&gt;self&lt;/code&gt;. In case of sklearn this is convention that any variable within in class with have a suffix &lt;code&gt;_&lt;/code&gt;. Here also we are saving, coefficients in &lt;code&gt;self.coeff_&lt;/code&gt;. Also, in &lt;code&gt;predict&lt;/code&gt; we are using &lt;code&gt;check_is_fitted(self)&lt;/code&gt; to check that the model has been fitted or not. I am using &lt;code&gt;check_array(X)&lt;/code&gt; in fit to ensure that the argument X is of type array.&lt;/p&gt;
&lt;p&gt;In the following block we are generating some synthetic data to fit the above model. Here &lt;code&gt;beta_star&lt;/code&gt; is the true parameter. &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;Y&lt;/code&gt; is the data on which we will fit the model and will try to estimate unknow &lt;code&gt;beta_star&lt;/code&gt; with derived &lt;code&gt;beta_hat&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def generate_data(m=100, n=20, sigma=5, density=0.2):
    &amp;quot;&amp;quot;&amp;quot;Generates data matrix X and observations Y.&amp;quot;&amp;quot;&amp;quot;
    np.random.seed(1)
    beta_star = np.random.randn(n)
    idxs = np.random.choice(range(n), int((1-density)*n), replace=False)
    for idx in idxs:
        beta_star[idx] = 0
    X = np.random.randn(m,n)
    Y = X.dot(beta_star) + np.random.normal(0, sigma, size=m)
    return X, Y, beta_star
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the following block we are generating the synthetic dataset, initializing lasso model with class &lt;code&gt;CVXSkLearnWrapper&lt;/code&gt; and &lt;code&gt;CVXSkLearnWrapper&lt;/code&gt;. After that we are executing &lt;code&gt;fit&lt;/code&gt; which will generate the coeffs &lt;code&gt;beta_hat&lt;/code&gt;, &lt;code&gt;predict&lt;/code&gt; will generate prediction &lt;code&gt;y_hat&lt;/code&gt; and &lt;code&gt;score&lt;/code&gt; will calculate R-sq between &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;y_hat&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X, y, _ = generate_data()
lasso = CVXSkLearnWrapper(alpha = 1.1)
model = lasso.fit(X, y)
model.predict(X)
model.score(X, y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Here we are using &lt;code&gt;RegressorMixin&lt;/code&gt; but depending on the model type it any can &lt;code&gt;ClassifierMixin&lt;/code&gt;, &lt;code&gt;RegressorMixin&lt;/code&gt;, &lt;code&gt;ClusterMixin&lt;/code&gt; or &lt;code&gt;TransformerMixin&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;All the methods and signature of them should be same if you are implementing more than one model to build a library.&lt;/li&gt;
&lt;li&gt;If you have &lt;code&gt;_&lt;/code&gt; prefix before any method in the model class, it will not be exposed. It will be considered as internal method and can be accessed with the class.&lt;/li&gt;
&lt;li&gt;If we inherit from &lt;code&gt;BaseEstimator&lt;/code&gt; &amp;amp; &lt;code&gt;RegressorMixin&lt;/code&gt; there will be a default score function but it can be overwritten.&lt;/li&gt;
&lt;li&gt;To change score function in hyper-parameter tuning you can use &lt;code&gt;make_scorer&lt;/code&gt; and &lt;code&gt;greater_is_better&lt;/code&gt; in it.&lt;/li&gt;
&lt;li&gt;Dynamic parsing of &lt;code&gt;args&lt;/code&gt; is possible in &lt;code&gt;__init__&lt;/code&gt; method using &lt;code&gt;inspect&lt;/code&gt; module,&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import inspect

def __init__(self, arg1, arg2, arg3, ..., argN):
    args, _, _, values = inspect.getargvalues(inspect.currentframe())
    values.pop(&amp;quot;self&amp;quot;)
    for arg, val in values.items():
        setattr(self, arg, val)
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;In this example, we are overwriting case &lt;code&gt;score&lt;/code&gt; function using &lt;code&gt;mean_absolute_percentage_error&lt;/code&gt;. Lets assume that for some reason we want to use &lt;code&gt;MAPE&lt;/code&gt; instate as default scoring method it can be useful. Other using &lt;code&gt;make_scorer&lt;/code&gt; can be used for any other custom score function or other sklearn score functions.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.utils.validation import check_X_y, check_array, check_is_fitted
import cvxpy as cp
import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_percentage_error

class CVXSkLearnWrapper(BaseEstimator, RegressorMixin):
    def __init__(self, alpha=1.0):
        self.alpha = alpha
        
    def _loss_fn(self, X, y, beta):
        return cp.norm2(X @ beta - y)**2

    def _regularizer(self, beta):
        return cp.norm1(beta)

    def _obj_fn(self, X, y, beta, lambd):
        return self._loss_fn(X, y, beta) + lambd * self._regularizer(beta)

    def _mse(self, X, Y, beta):
        return (1.0 / X.shape[0]) * self._loss_fn(X, Y, beta).value

    def fit(self, X, y):
        X, y = check_X_y(X, y)
        n = X.shape[1]
        beta = cp.Variable(n)
        lambd = cp.Parameter(nonneg=True)
        problem = cp.Problem(cp.Minimize(self._obj_fn(X, y, beta, lambd)))
        lambd.value = self.alpha
        problem.solve()
        self.coeff_ = beta.value
        self.intercept_ = 0.0
        return self
    
    def predict(self, X):
        check_is_fitted(self)
        X = check_array(X)
        return X @ self.coeff_

    def score(self, X, y):
        y_hat = self.predict(X)
        return mean_absolute_percentage_error(y, y_hat)
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;All the hyper-parameters (not derived from data) has to be initialized in &lt;code&gt;__init__&lt;/code&gt; method. Any model parameters (derived from data) must be initialized in &lt;code&gt;fit&lt;/code&gt;. Variable names in init should be always same as arg name and variables in fit should always have a suffix &lt;code&gt;_&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt; are mandatory methods in &lt;code&gt;BaseEstimator&lt;/code&gt; class.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Reference:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cvxpy.org/examples/machine_learning/lasso_regression.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LASSO regression using CVXPY&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://danielhnyk.cz/creating-your-own-estimator-scikit-learn/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Creating your own estimator in scikit-learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://acme.byu.edu/00000179-d3f1-d7a6-a5fb-ffff6a2a0000/sklearn-1-pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Introduction to Scikit-Learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://stackoverflow.com/questions/32401493/how-to-create-customize-your-own-scorer-function-in-scikit-learn&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How to create/customize your own scorer function in scikit-learn?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scikit-learn.org/stable/modules/model_evaluation.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Metrics and scoring: quantifying the quality of predictions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>RecSys for B2B platform</title>
      <link>https://academic-demo.netlify.app/project/internal-project/</link>
      <pubDate>Sun, 18 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/project/internal-project/</guid>
      <description>&lt;p&gt;I am working on &lt;code&gt;RecSys&lt;/code&gt; to generate product recommendations for ABI&amp;rsquo;s B2B platform &lt;code&gt;BEEs&lt;/code&gt;. Some of the challenges involved in the project include building &lt;code&gt;AutoML&lt;/code&gt; for best hyper-parameter selection, distributed model training. feature store integration, building a python library for curated ML models with default configs, deployment of models in cloud native compute and many more. Super excited to work in this work stream with an amazing team.&lt;/p&gt;
&lt;h3 id=&#34;__algorithm-related-challenges__&#34;&gt;&lt;strong&gt;Algorithm related challenges:&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cross validation:&lt;/strong&gt; How to perform cross validation for &lt;code&gt;RecSys&lt;/code&gt;. How to link statistical metrics with business KPIs. Determining weighage between model goodness of fit and business KPIs. How to create a scoring function which can compare between different models during cross validation. Managing splitting strategy to ensure that models are comparable.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model selection:&lt;/strong&gt; Single model or market-based model or hybrid model - combined of two or more models? Time/Sequence based models(&lt;code&gt;LSTM&lt;/code&gt;/&lt;code&gt;GRU&lt;/code&gt;)?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hyper parameter tuning:&lt;/strong&gt; What can be the preferable hyper-parameter tuning framework, which can support &lt;code&gt;GPU&lt;/code&gt; (Wide and Deep), Spark (&lt;code&gt;ALS&lt;/code&gt;) and CPU (&lt;code&gt;SAR&lt;/code&gt; etc.).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;KPIs:&lt;/strong&gt; Evaluate existing KPIs such as &lt;code&gt;Map@K&lt;/code&gt;, &lt;code&gt;NDCG@K&lt;/code&gt; and improve if possible.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hybrid model or mixture of model:&lt;/strong&gt; Also, what type of hybrid - sequential, parallel or weighted? As of now, two use case (conceptually)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;AutoML:&lt;/strong&gt; Example of AutoML for multi-country setup (including hybrid model, hyper-parameter tuning) with recommended tech stack.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model drift, data drift, retraining and model monitoring:&lt;/strong&gt; How to build a framework which can be integrated with the python library to detect model drift, data drift, retraining requirements and monitor generated results in online and offline models.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Others:&lt;/strong&gt; Backtesting, AB testing, linking online and offline evaluation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;__programming--infra-related-challenges__&#34;&gt;&lt;strong&gt;Programming &amp;amp; Infra related challenges:&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Code spaces:&lt;/strong&gt; How a developer can use code space for CPU based workflow for day-to-day development. Managing multiple envs base of &lt;code&gt;Spark/GPU/CPU&lt;/code&gt; dependencies using &lt;code&gt;devcontainer&lt;/code&gt;. Can the same image be used in &lt;code&gt;AML/ADB&lt;/code&gt;?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;AML&lt;/code&gt; + VS Code/Code Space Integration:&lt;/strong&gt; Attaching &lt;code&gt;AML&lt;/code&gt; compute to VS Code as terminal and &lt;code&gt;jupyter&lt;/code&gt; kernel. Run experiments in AML without leaving &lt;code&gt;VS Code/Code spaces&lt;/code&gt;. Triggering multiple concurrent jobs (not always same as distributed model training. Some of our models are classical models which we are running multiple times as embarrassingly parallel workload) in &lt;code&gt;AML&lt;/code&gt; from &lt;code&gt;VS Code&lt;/code&gt; which can scale in multiple nodes to run different models and return results in a fan out fan in pattern to a cloud storage. (One additional information here is we want to leverage all the cores within a node using &lt;code&gt;joblib&lt;/code&gt;, hence the auto scaling we are expecting is at node level for a given threshold) &lt;code&gt;mlflow&lt;/code&gt; integration with VS Code and AML.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;ADB&lt;/code&gt; + VS Code/Code Space Integration:&lt;/strong&gt; Run experiment in ADB without leaving VS Code/Code spaces.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Debugging:&lt;/strong&gt; Using VS Code visual debugger in a distributed workflow in &lt;code&gt;AML&lt;/code&gt; &amp;amp; &lt;code&gt;ADB&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Observability:&lt;/strong&gt; Monitoring aggregated logs from different nodes in &lt;code&gt;VS Code&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Testing:&lt;/strong&gt; How to run property based testing for ML models in distributed compute environments.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Library:&lt;/strong&gt; Managing multiple dependencies such as &lt;code&gt;pyspark&lt;/code&gt;, &lt;code&gt;GPU&lt;/code&gt; and &lt;code&gt;CPU&lt;/code&gt; level system dependencies. Usage of &lt;code&gt;JIT&lt;/code&gt; within and across models taking execution infra into account. Making library infra agnostic.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you are excited about solving above mentioned challenges feel free to reach out to me.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
