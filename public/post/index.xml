<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Aritra Biswas</title>
    <link>http://localhost:4321/post/</link>
      <atom:link href="http://localhost:4321/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sun, 09 Mar 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:4321/media/icon_hu966111359855668688.png</url>
      <title>Posts</title>
      <link>http://localhost:4321/post/</link>
    </image>
    
    <item>
      <title>On LLM reproducibility</title>
      <link>http://localhost:4321/post/on-llm-reproducibility/</link>
      <pubDate>Sun, 09 Mar 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/on-llm-reproducibility/</guid>
      <description>&lt;p&gt;One of the major issue with LLM output is its consistency and reproducibility. Here is kind of a hacky work around to make output from a LLM consistent (kind of). Also, a few observation regarding the same. Here I am following a MSFT article linked in the reference. Need to use the a GPT 4o model with version &lt;code&gt;2024-05-13&lt;/code&gt; for this. Also note here using AOAI API version &lt;code&gt;2024-10-21&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;setting-up-two-instances-of-llm-with-different-configs&#34;&gt;Setting up two instances of LLM with different configs&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-txt&#34;&gt;Deployment info
Name: gpt-4o
Model name: gpt-4o
Model version: 2024-05-13
Azure OAI API version: 2024-10-21
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-env&#34;&gt;AZURE_OPENAI_API_KEY=&amp;lt;AZURE_OPENAI_API_KEY&amp;gt;
AZURE_OPENAI_ENDPOINT=&amp;lt;AZURE_OPENAI_ENDPOINT&amp;gt;
AZURE_OPENAI_API_VERSION=&amp;quot;2024-10-21&amp;quot;
AZURE_OPENAI_GPT4O_MODEL_NAME=&amp;quot;gpt-4o&amp;quot;
AZURE_OPENAI_GPT4O_DEPLOYMENT_NAME=&amp;quot;gpt-4o&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from langchain_openai import AzureChatOpenAI
import pandas as pd
from dotenv import load_dotenv
import time
load_dotenv()
consistent_llm = AzureChatOpenAI(
    api_key=AZURE_OPENAI_API_KEY,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    azure_deployment=AZURE_OPENAI_GPT4O_DEPLOYMENT_NAME,
    api_version=AZURE_OPENAI_API_VERSION,
    temperature=0,
    seed=42,
    max_tokens=50
)
less_consistent_llm = AzureChatOpenAI(
    api_key = AZURE_OPENAI_API_KEY,
    azure_endpoint = AZURE_OPENAI_ENDPOINT,
    azure_deployment=AZURE_OPENAI_GPT4O_DEPLOYMENT_NAME,
    api_version=AZURE_OPENAI_API_VERSION,
)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;generation-without-any-context&#34;&gt;Generation without any context&lt;/h3&gt;
&lt;p&gt;In this experiment generating 50 samples where the generation is not grounded on context. In way it is more free to choose the next word.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;consistent_arr_res = []
less_consistent_arr_res = []
messages = [
    {&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;You are a helpful assistant.&amp;quot;},
    {
        &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;,
        &amp;quot;content&amp;quot;: &amp;quot;Tell me a story about how the universe began? in 15 words&amp;quot;,
    },
]

for idx in range(50):
    print(f&amp;quot;Story Version {idx + 1}\n---&amp;quot;)
    consistent_res = consistent_llm.invoke(input=messages)
    less_consistent_res = less_consistent_llm.invoke(input=messages)
    less_consistent_arr_res.append(less_consistent_res.content)
    consistent_arr_res.append(consistent_res.content)
    print(&amp;quot;---\n&amp;quot;)
    del consistent_res
    del less_consistent_res

consistent_df = pd.DataFrame(consistent_arr_res, columns = [&#39;res&#39;])
less_consistent_df = pd.DataFrame(less_consistent_arr_res, columns = [&#39;res&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Observation in 50 generation with consistent LLM (with seed 42, max_token 50 and temperature 0) it has generated the below table.&lt;/p&gt;
&lt;h4 id=&#34;consistent-dataframe-1&#34;&gt;Consistent dataframe 1&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(consistent_df.value_counts().to_frame().to_markdown())
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;&lt;/th&gt;
          &lt;th style=&#34;text-align: right&#34;&gt;count&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;(&amp;lsquo;In a cosmic explosion, the universe expanded, stars formed, and life eventually emerged.&amp;rsquo;,)&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;19&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;(&amp;ldquo;In a vast void, a singularity exploded, birthing stars, galaxies, and the universe&amp;rsquo;s endless wonders.&amp;rdquo;,)&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;15&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;(&amp;lsquo;In a cosmic explosion, energy and matter formed stars, galaxies, and life, creating our universe.&amp;rsquo;,)&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;6&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;(&amp;lsquo;The universe began with a massive explosion, expanding rapidly, forming stars, galaxies, and cosmic wonders.&amp;rsquo;,)&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;3&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;(&amp;lsquo;The universe began with a massive explosion, expanding rapidly, forming stars, galaxies, and everything else.&amp;rsquo;,)&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;3&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;(&amp;lsquo;The universe began with a massive explosion, expanding rapidly, forming stars, galaxies, and life.&amp;rsquo;,)&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;3&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;(&amp;ldquo;In a vast void, a singularity exploded, birthing stars, galaxies, and the universe&amp;rsquo;s wonders.&amp;rdquo;,)&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;1&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;less-consistent-dataframe-1&#34;&gt;Less Consistent dataframe 1&lt;/h4&gt;
&lt;p&gt;Observation in 50 generation with inconsistent LLM it has generated the below table.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(less_consistent_df.value_counts().to_frame()/head(5).to_markdown())
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;&lt;/th&gt;
          &lt;th style=&#34;text-align: right&#34;&gt;count&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;(&amp;lsquo;A cosmic explosion birthed stars, planets, and galaxies, igniting the endless dance of creation.&amp;rsquo;,)&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;1&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;(&amp;lsquo;Stars ignited, galaxies whirled; from cosmic dawn, life sprung, evolving endlessly in infinite wonder.&amp;rsquo;,)&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;1&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;(&amp;lsquo;In the beginning, a Big Bang created stars, galaxies, and everything we know today.&amp;rsquo;,)&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;1&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;(&amp;lsquo;In the beginning, a cosmic explosion birthed stars, planets, and galaxies, igniting infinite possibilities.&amp;rsquo;,)&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;1&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;(&amp;lsquo;In the beginning, a massive explosion unleashed energy, forming stars, planets, and endless possibilities.&amp;rsquo;,)&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;1&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here showing just to 5 sample. But all 50 generation are unqiue.&lt;/p&gt;
&lt;h3 id=&#34;generation-with-context&#34;&gt;Generation with context&lt;/h3&gt;
&lt;p&gt;Now, we will ground generation using some context. Here is a short summary of short stories from wikipedia.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text = &amp;quot;&amp;quot;&amp;quot;A short story is a piece of prose fiction. It can typically be read in a single sitting and focuses on a self-contained incident or series of linked incidents, with the intent of evoking a single effect or mood. The short story is one of the oldest types of literature and has existed in the form of legends, mythic tales, folk tales, fairy tales, tall tales, fables, and anecdotes in various ancient communities around the world. The modern short story developed in the early 19th century.[1]
Definition
The short story is a crafted form in its own right. Short stories make use of plot, resonance and other dynamic components as in a novel, but typically to a lesser degree. While the short story is largely distinct from the novel or novella/short novel, authors generally draw from a common pool of literary techniques.[citation needed] The short story is sometimes referred to as a genre.[2]
Determining what exactly defines a short story remains problematic.[3] A classic definition of a short story is that one should be able to read it in one sitting, a point most notably made in Edgar Allan Poe&#39;s essay &amp;quot;The Philosophy of Composition&amp;quot; (1846).[4] H. G. Wells described the purpose of the short story as &amp;quot;The jolly art, of making something very bright and moving; it may be horrible or pathetic or funny or profoundly illuminating, having only this essential, that it should take from fifteen to fifty minutes to read aloud.&amp;quot;[5] According to William Faulkner, a short story is character-driven and a writer&#39;s job is to &amp;quot;...trot along behind him with a paper and pencil trying to keep up long enough to put down what he says and does.&amp;quot;[6]
Some authors have argued that a short story must have a strict form. Somerset Maugham thought that the short story &amp;quot;must have a definite design, which includes a point of departure, a climax and a point of test; in other words, it must have a plot&amp;quot;.[5] Hugh Walpole had a similar view: &amp;quot;A story should be a story; a record of things happening full of incidents, swift movements, unexpected development, leading through suspense to a climax and a satisfying denouement.&amp;quot;[5]
This view of the short story as a finished product of art is however opposed by Anton Chekhov, who thought that a story should have neither a beginning nor an end. It should just be a &amp;quot;slice of life&amp;quot;, presented suggestively. In his stories, Chekhov does not round off the end but leaves it to the readers to draw their own conclusions.[5]
Sukumar Azhikode defined a short story as &amp;quot;a brief prose narrative with an intense episodic or anecdotal effect&amp;quot;.[3] Flannery O&#39;Connor emphasized the need to consider what is exactly meant by the descriptor short.[7] Short story writers may define their works as part of the artistic and personal expression of the form. They may also attempt to resist categorization by genre and fixed formation.[5]
William Boyd, a British author and short story writer, has said:
[a short story] seem[s] to answer something very deep in our nature as if, for the duration of its telling, something special has been created, some essence of our experience extrapolated, some temporary sense has been made of our common, turbulent journey towards the grave and oblivion.[8]
In the 1880s, the term &amp;quot;short story&amp;quot; acquired its modern meaning – having initially referred to children&#39;s tales.[9] During the early to mid-20th century, the short story underwent expansive experimentation which further hindered attempts to comprehensively provide a definition.[3] Longer stories that cannot be called novels are sometimes considered &amp;quot;novellas&amp;quot; or novelettes and, like short stories, may be collected into the more marketable form of &amp;quot;collections&amp;quot;. Around the world, the modern short story is comparable to lyrics, dramas, novels and essays – although examination of it as a major literary form remains diminished.[3][10]&amp;quot;&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, asking LLMs to generate a summary using the given context.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;consistent_arr_res_summary = []
less_consistent_arr_res_summary = []
messages = [
    {&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;You are a helpful assistant.&amp;quot;},
    {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: f&amp;quot;summarize this text in 15 words: {text}&amp;quot;},
]
for i in range(50):
    print(f&amp;quot;Story Version {i + 1}\n---&amp;quot;)
    consistent_res_summary = consistent_llm.invoke(input=messages)
    less_consistent_res_summary = less_consistent_llm.invoke(input=messages)
    less_consistent_arr_res_summary.append(less_consistent_res_summary.content)
    consistent_arr_res_summary.append(consistent_res_summary.content)
    print(&amp;quot;---\n&amp;quot;)
    del consistent_res_summary
    del less_consistent_res_summary
    time.sleep(1)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;consistent-dataframe-2&#34;&gt;Consistent dataframe 2&lt;/h4&gt;
&lt;p&gt;Observation in 50 generation with consistent LLM (with seed 42, max_token 50 and temperature 0) it has generated the below table. As you can see it is way more consistent. 46 time it has generated same output and 4 times the output is different but the main different in these two is of a single charecter.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(consistent_df_summary.value_counts().to_frame().to_markdown())
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;&lt;/th&gt;
          &lt;th style=&#34;text-align: right&#34;&gt;count&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;(&amp;lsquo;A short story is a brief, self-contained prose fiction, read in one sitting, evoking a mood.&amp;rsquo;,)&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;46&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;(&amp;lsquo;A short story is a brief, self-contained prose fiction, read in one sitting, evoking mood.&amp;rsquo;,)&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;4&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;less-consistent-dataframe-2&#34;&gt;Less Consistent dataframe 2&lt;/h4&gt;
&lt;p&gt;Observation in 50 generation with inconsistent LLM it has generated the below table.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(less_consistent_df_summary.value_counts()head(5).to_frame().to_markdown())
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;&lt;/th&gt;
          &lt;th style=&#34;text-align: right&#34;&gt;count&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;(&amp;lsquo;A short story is a brief piece of prose fiction read in a single sitting.&amp;rsquo;,)&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;1&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;(&amp;lsquo;A short story is a concise prose narrative, focusing on a single effect or mood.&amp;rsquo;,)&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;1&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;(&amp;lsquo;A short story is a brief, self-contained prose fiction focusing on a single effect or mood.&amp;rsquo;,)&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;1&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;(&amp;lsquo;A short story is a brief, self-contained prose fiction focusing on a single incident or mood.&amp;rsquo;,)&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;1&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;(&amp;lsquo;A short story is a brief, self-contained prose fiction read in one sitting, evoking a single effect.&amp;rsquo;,)&lt;/td&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;1&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Again this is a sample of head 5. But as you can see that all the 50 generations are unique and different from one another.&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/reproducible-output?tabs=pyton&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learn how to use reproducible output&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Short_story&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Short story&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Exponential Smoothing using Scikit-Learn wrapper &amp; statsmodels</title>
      <link>http://localhost:4321/post/exponential-smoothing-using-scikit-learn-wrapper-statsmodels/</link>
      <pubDate>Tue, 20 Dec 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/exponential-smoothing-using-scikit-learn-wrapper-statsmodels/</guid>
      <description>&lt;p&gt;As we discussed in previous post, in this series we will be working on different type of regression problem and will try to parse them as sklearn model objects. Here are we will be working with &lt;code&gt;ExponentialSmoothing&lt;/code&gt; from &lt;code&gt;statsmodels&lt;/code&gt; library. This is one of the most common time series model used in general. &lt;code&gt;statsmodels&lt;/code&gt; is heavily influence by R and its convention. If you take a deep dive into it&amp;rsquo;s functionality of or formula based approach in case of GLM, it is quite evident including the summary function.&lt;/p&gt;
&lt;p&gt;Here in this post our objective will be to take this &lt;code&gt;ExponentialSmoothing&lt;/code&gt; from &lt;code&gt;statsmodels&lt;/code&gt; and build a model class which looks like sklearn model and the method signatures are same in nature so there is less learning curve involved when there is a context switch or in this case a user is trying to build different models.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from statsmodels.tsa.holtwinters import ExponentialSmoothing
from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.utils.validation import check_X_y, check_array, check_is_fitted
from sklearn.metrics import mean_absolute_percentage_error
import numpy as np
import pandas as pd
import inspect

class HWTimeSeriesSkLearnWrapper(BaseEstimator, RegressorMixin):
    def __init__(
        self,
        trend=None,
        damped_trend=False,
        seasonal=None,
        seasonal_periods=None,
        use_boxcox=False,
        optimized=True,
        remove_bias=False,
        initialization_method = None,
        model=ExponentialSmoothing,
    ):
        args, _, _, values = inspect.getargvalues(inspect.currentframe())
        values.pop(&amp;quot;self&amp;quot;)
        for arg, val in values.items():
            setattr(self, arg, val)

    def fit(self, X, y = None):
        if not X.empty:
            self.X = X
        self.model_ = self.model(
            self.X,
            trend=self.trend,
            seasonal=self.seasonal,
            seasonal_periods=self.seasonal_periods,
            damped_trend=self.damped_trend,
            use_boxcox=self.use_boxcox,
        )
        self.result_ = self.model_.fit(
            optimized=self.optimized, remove_bias=self.remove_bias
        )
        self.level = self.result_.level
        self.trend = self.result_.trend
        self.season = self.result_.season
        self.fitted_values = self.result_.fittedvalues
        return self

    def predict(self, X, y = None):
        check_is_fitted(self, &amp;quot;result_&amp;quot;)
        self.df = self.result_.forecast(X.index.size).to_frame()
        self.df.columns = [f&amp;quot;predicted_{name}&amp;quot; for name in X.columns]
        self.df.index.name = X.index.name
        return self.df

    def score(self, X, y):
        y_true = X.to_numpy().flatten()
        y_hat = self.predict(X).to_numpy().flatten()
        return mean_absolute_percentage_error(y_true= y_true, y_pred=y_hat)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see in the above block, I have come up with this structure which can take all the args of &lt;code&gt;__init__&lt;/code&gt; and &lt;code&gt;fit&lt;/code&gt; of &lt;code&gt;ExponentialSmoothing&lt;/code&gt; while initializing the wrapper class &lt;code&gt;HWTimeSeriesSkLearnWrapper&lt;/code&gt;. Note, these are not all the exhaustive set of parameter. You can take a look into the source code of the model class from &lt;code&gt;statsmodels&lt;/code&gt; and figure out what are the other parameters which can be helpful for your context.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;args, _, _, values = inspect.getargvalues(inspect.currentframe())
values.pop(&amp;quot;self&amp;quot;)
for arg, val in values.items():
    setattr(self, arg, val)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above chunk of the code will help to initialize arbitrary number of parameters for your model and in subsequent methods such as initialization or fit you can pass the parameter values from self. In the next block, I am downloading the data and creating a train test split. Note, in case of time series we need to remember two things,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Train test split can not be random&lt;/li&gt;
&lt;li&gt;There is no such concept of train and test data. Index is kind of your core feature along with meta features which can be derived from target.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The catch here is we need to create train and test in a way it can look like sklearn model but under the hood it should be able to use the same datasets for building times series model using statsmodels or any other library you may use.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def get_and_split_airline_data(test_size = 0.2, date_col = &amp;quot;month&amp;quot;, date_format = &amp;quot;%Y-%m&amp;quot;, date_freq = &amp;quot;MS&amp;quot;):
    df = pd.read_csv(&amp;quot;https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv&amp;quot;)
    df.columns = df.columns.str.lower()
    if date_col in df.columns:
        df[date_col] = pd.to_datetime(df[date_col], format = date_format)
        df = df.sort_values(date_col).set_index(date_col)
        df.index.freq = date_freq
    train_size = int(df.shape[0] * (1 - test_size))
    X_train = df.iloc[:train_size]
    X_test = df.iloc[train_size:]
    y_train = X_train.copy()
    y_test = X_test.copy()
    return X_train,X_test,y_train, y_test
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, once we have the data we can use &lt;code&gt;fit&lt;/code&gt;, &lt;code&gt;predict&lt;/code&gt; and &lt;code&gt;score&lt;/code&gt; to calculate the required values.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_train,X_test,y_train, y_test = get_and_split_airline_data()
model = HWTimeSeriesSkLearnWrapper(trend = &amp;quot;add&amp;quot;, damped_trend = True, seasonal = &amp;quot;add&amp;quot;, seasonal_periods = 12)
model.fit(X_train, y_train)
model.predict(X_test)
model.score(X_test, y_test)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>LASSO using Scikit-Learn wrapper &amp; CVXPY</title>
      <link>http://localhost:4321/post/lasso-using-scikit-learn-wrapper-and-cvxpy/</link>
      <pubDate>Sun, 18 Dec 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/lasso-using-scikit-learn-wrapper-and-cvxpy/</guid>
      <description>&lt;p&gt;Those who are working in ML space of sometime must be aware that there are multiple python libraries out there which support different algorithms/models. When you are trying to implement best model it is not possible to use just a single library or even a single language and compute infra. Also, note all the libraries are do not have the same method, signature and input and output. Hence if you are working on a library which is collection of multiple models from different library one good starting point can be to standardize you class, methods, signature and input and output. It helps us to avoid confusion, make things consistent and as result, when you are onboarding a new user, it takes them less time to learn things.&lt;/p&gt;
&lt;p&gt;At high level, it may sound easy to create common API structure for all the models but it is not. Many models take input in form of &lt;code&gt;np.ndarray&lt;/code&gt;, &lt;code&gt;pd.Series&lt;/code&gt;, &lt;code&gt;pd.DataFrame&lt;/code&gt;, &lt;code&gt;xarray&lt;/code&gt; objects and some libraries implement their own data object. To handle this project my current go to approach is to parse the inner working of a model in a sklearn wrapper with common methods, signature and input and output. In this example, we will see how we can do this using &lt;code&gt;cvxpy&lt;/code&gt; as optimizer and &lt;code&gt;sklearn&lt;/code&gt; wrapper to generate a &lt;code&gt;sklearn&lt;/code&gt; model object. Same thing can be done for &lt;code&gt;statsmodels&lt;/code&gt;, &lt;code&gt;pytorch&lt;/code&gt;, &lt;code&gt;tensorflow&lt;/code&gt; or any other custom logic.&lt;/p&gt;
&lt;p&gt;To start with we can create a &lt;code&gt;conda&lt;/code&gt; virtual env (assuming &lt;code&gt;conda&lt;/code&gt; is already installed. Otherwise install &lt;code&gt;conda&lt;/code&gt; python 3.8 first and then revisit this.). Run the following block to create a virtual env in &lt;code&gt;conda&lt;/code&gt; and install the required libraries,&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n sklearn_wrapper python=3.8 -y
conda activate sklearn_wrapper
pip install scikit-learn cvxpy jupyter notebook pandas
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once this is done, we are good to start. Open a &lt;code&gt;jupyter notebook&lt;/code&gt; and execute the following lines in a cell,&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.utils.validation import check_X_y, check_array, check_is_fitted
import cvxpy as cp
import numpy as np
import pandas as pd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we are importing &lt;code&gt;BaseEstimator&lt;/code&gt; and &lt;code&gt;RegressorMixin&lt;/code&gt; which our custom model class will inherit from. &lt;code&gt;BaseEstimator&lt;/code&gt; will have default score method which can be over written and &lt;code&gt;RegressorMixin&lt;/code&gt; will have &lt;code&gt;get_params&lt;/code&gt; and &lt;code&gt;set_params&lt;/code&gt; methods which can be useful later while execution to change to value of a model parameter. Other than these we are importing &lt;code&gt;check_X_y&lt;/code&gt;, &lt;code&gt;check_array&lt;/code&gt; and &lt;code&gt;check_is_fitted&lt;/code&gt; from &lt;code&gt;sklearn&lt;/code&gt; which will be used to check if the X and y we are passing is of sklearn convention or not, the variable we are passing is of &lt;code&gt;array&lt;/code&gt; type of not and &lt;code&gt;check_is_fitted&lt;/code&gt; is to prevent running&lt;code&gt;predict&lt;/code&gt; before &lt;code&gt;fit&lt;/code&gt;. If we run predict, any model will expect &lt;code&gt;coeff&lt;/code&gt; or &lt;code&gt;weight&lt;/code&gt; to make any prediction, until we run &lt;code&gt;fit&lt;/code&gt; methods, these values will not be generated.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class CVXSkLearnWrapper(BaseEstimator, RegressorMixin):
    def __init__(self, alpha=1.0):
        self.alpha = alpha
        
    def _loss_fn(self, X, y, beta):
        return cp.norm2(X @ beta - y)**2

    def _regularizer(self, beta):
        return cp.norm1(beta)

    def _obj_fn(self, X, y, beta, lambd):
        return self._loss_fn(X, y, beta) + lambd * self._regularizer(beta)

    def _mse(self, X, Y, beta):
        return (1.0 / X.shape[0]) * self._loss_fn(X, Y, beta).value

    def fit(self, X, y):
        X, y = check_X_y(X, y)
        n = X.shape[1]
        beta = cp.Variable(n)
        lambd = cp.Parameter(nonneg=True)
        problem = cp.Problem(cp.Minimize(self._obj_fn(X, y, beta, lambd)))
        lambd.value = self.alpha
        problem.solve()
        self.coeff_ = beta.value
        self.intercept_ = 0.0
        return self
    
    def predict(self, X):
        check_is_fitted(self)
        X = check_array(X)
        return X @ self.coeff_
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the implementation LASSO using &lt;code&gt;CVXPY&lt;/code&gt; as &lt;code&gt;SKLearn&lt;/code&gt; model. Here in &lt;code&gt;__init__&lt;/code&gt; method we are taking &lt;code&gt;alpha&lt;/code&gt; as user input. Note, all the argument in &lt;code&gt;__init__&lt;/code&gt; method should have a default values. While storing them in &lt;code&gt;self&lt;/code&gt; the argument name of &lt;code&gt;__init__&lt;/code&gt; and reference in &lt;code&gt;self&lt;/code&gt; should be same. For example if we are using &lt;code&gt;__init__(self, alpha=1.0)&lt;/code&gt; then it must be store in &lt;code&gt;self&lt;/code&gt; as &lt;code&gt;self.alpha = alpha&lt;/code&gt;. This is a parameter which can be changed during execution using &lt;code&gt;get_params&lt;/code&gt; and &lt;code&gt;set_params&lt;/code&gt; method. To know about this optimization more check &lt;a href=&#34;https://www.cvxpy.org/examples/machine_learning/lasso_regression.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this&lt;/a&gt; link. Note, here inside &lt;code&gt;fit&lt;/code&gt; method we have &lt;code&gt;X, y = check_X_y(X, y)&lt;/code&gt; this is not ensure that the &lt;code&gt;X, y&lt;/code&gt; variable we have passed to the &lt;code&gt;fit&lt;/code&gt; method is correct with respect to data type and share. Also, &lt;code&gt;fit&lt;/code&gt; method of &lt;code&gt;sklearn&lt;/code&gt; always return &lt;code&gt;self&lt;/code&gt;. In case of sklearn this is convention that any variable within in class with have a suffix &lt;code&gt;_&lt;/code&gt;. Here also we are saving, coefficients in &lt;code&gt;self.coeff_&lt;/code&gt;. Also, in &lt;code&gt;predict&lt;/code&gt; we are using &lt;code&gt;check_is_fitted(self)&lt;/code&gt; to check that the model has been fitted or not. I am using &lt;code&gt;check_array(X)&lt;/code&gt; in fit to ensure that the argument X is of type array.&lt;/p&gt;
&lt;p&gt;In the following block we are generating some synthetic data to fit the above model. Here &lt;code&gt;beta_star&lt;/code&gt; is the true parameter. &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;Y&lt;/code&gt; is the data on which we will fit the model and will try to estimate unknow &lt;code&gt;beta_star&lt;/code&gt; with derived &lt;code&gt;beta_hat&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def generate_data(m=100, n=20, sigma=5, density=0.2):
    &amp;quot;&amp;quot;&amp;quot;Generates data matrix X and observations Y.&amp;quot;&amp;quot;&amp;quot;
    np.random.seed(1)
    beta_star = np.random.randn(n)
    idxs = np.random.choice(range(n), int((1-density)*n), replace=False)
    for idx in idxs:
        beta_star[idx] = 0
    X = np.random.randn(m,n)
    Y = X.dot(beta_star) + np.random.normal(0, sigma, size=m)
    return X, Y, beta_star
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the following block we are generating the synthetic dataset, initializing lasso model with class &lt;code&gt;CVXSkLearnWrapper&lt;/code&gt; and &lt;code&gt;CVXSkLearnWrapper&lt;/code&gt;. After that we are executing &lt;code&gt;fit&lt;/code&gt; which will generate the coeffs &lt;code&gt;beta_hat&lt;/code&gt;, &lt;code&gt;predict&lt;/code&gt; will generate prediction &lt;code&gt;y_hat&lt;/code&gt; and &lt;code&gt;score&lt;/code&gt; will calculate R-sq between &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;y_hat&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X, y, _ = generate_data()
lasso = CVXSkLearnWrapper(alpha = 1.1)
model = lasso.fit(X, y)
model.predict(X)
model.score(X, y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Here we are using &lt;code&gt;RegressorMixin&lt;/code&gt; but depending on the model type it any can &lt;code&gt;ClassifierMixin&lt;/code&gt;, &lt;code&gt;RegressorMixin&lt;/code&gt;, &lt;code&gt;ClusterMixin&lt;/code&gt; or &lt;code&gt;TransformerMixin&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;All the methods and signature of them should be same if you are implementing more than one model to build a library.&lt;/li&gt;
&lt;li&gt;If you have &lt;code&gt;_&lt;/code&gt; prefix before any method in the model class, it will not be exposed. It will be considered as internal method and can be accessed with the class.&lt;/li&gt;
&lt;li&gt;If we inherit from &lt;code&gt;BaseEstimator&lt;/code&gt; &amp;amp; &lt;code&gt;RegressorMixin&lt;/code&gt; there will be a default score function but it can be overwritten.&lt;/li&gt;
&lt;li&gt;To change score function in hyper-parameter tuning you can use &lt;code&gt;make_scorer&lt;/code&gt; and &lt;code&gt;greater_is_better&lt;/code&gt; in it.&lt;/li&gt;
&lt;li&gt;Dynamic parsing of &lt;code&gt;args&lt;/code&gt; is possible in &lt;code&gt;__init__&lt;/code&gt; method using &lt;code&gt;inspect&lt;/code&gt; module,&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import inspect

def __init__(self, arg1, arg2, arg3, ..., argN):
    args, _, _, values = inspect.getargvalues(inspect.currentframe())
    values.pop(&amp;quot;self&amp;quot;)
    for arg, val in values.items():
        setattr(self, arg, val)
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;In this example, we are overwriting case &lt;code&gt;score&lt;/code&gt; function using &lt;code&gt;mean_absolute_percentage_error&lt;/code&gt;. Lets assume that for some reason we want to use &lt;code&gt;MAPE&lt;/code&gt; instate as default scoring method it can be useful. Other using &lt;code&gt;make_scorer&lt;/code&gt; can be used for any other custom score function or other sklearn score functions.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.utils.validation import check_X_y, check_array, check_is_fitted
import cvxpy as cp
import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_percentage_error

class CVXSkLearnWrapper(BaseEstimator, RegressorMixin):
    def __init__(self, alpha=1.0):
        self.alpha = alpha
        
    def _loss_fn(self, X, y, beta):
        return cp.norm2(X @ beta - y)**2

    def _regularizer(self, beta):
        return cp.norm1(beta)

    def _obj_fn(self, X, y, beta, lambd):
        return self._loss_fn(X, y, beta) + lambd * self._regularizer(beta)

    def _mse(self, X, Y, beta):
        return (1.0 / X.shape[0]) * self._loss_fn(X, Y, beta).value

    def fit(self, X, y):
        X, y = check_X_y(X, y)
        n = X.shape[1]
        beta = cp.Variable(n)
        lambd = cp.Parameter(nonneg=True)
        problem = cp.Problem(cp.Minimize(self._obj_fn(X, y, beta, lambd)))
        lambd.value = self.alpha
        problem.solve()
        self.coeff_ = beta.value
        self.intercept_ = 0.0
        return self
    
    def predict(self, X):
        check_is_fitted(self)
        X = check_array(X)
        return X @ self.coeff_

    def score(self, X, y):
        y_hat = self.predict(X)
        return mean_absolute_percentage_error(y, y_hat)
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;All the hyper-parameters (not derived from data) has to be initialized in &lt;code&gt;__init__&lt;/code&gt; method. Any model parameters (derived from data) must be initialized in &lt;code&gt;fit&lt;/code&gt;. Variable names in init should be always same as arg name and variables in fit should always have a suffix &lt;code&gt;_&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fit&lt;/code&gt; and &lt;code&gt;predict&lt;/code&gt; are mandatory methods in &lt;code&gt;BaseEstimator&lt;/code&gt; class.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Reference:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cvxpy.org/examples/machine_learning/lasso_regression.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LASSO regression using CVXPY&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://danielhnyk.cz/creating-your-own-estimator-scikit-learn/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Creating your own estimator in scikit-learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://acme.byu.edu/00000179-d3f1-d7a6-a5fb-ffff6a2a0000/sklearn-1-pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Introduction to Scikit-Learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://stackoverflow.com/questions/32401493/how-to-create-customize-your-own-scorer-function-in-scikit-learn&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How to create/customize your own scorer function in scikit-learn?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scikit-learn.org/stable/modules/model_evaluation.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Metrics and scoring: quantifying the quality of predictions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Avoid relative path import hell in python</title>
      <link>http://localhost:4321/post/avoid-relative-path-import-hell-in-python/</link>
      <pubDate>Sat, 18 Dec 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/avoid-relative-path-import-hell-in-python/</guid>
      <description>&lt;h1 id=&#34;__exploring-poetry-for-dependency-management-in-python__&#34;&gt;&lt;strong&gt;Exploring &lt;code&gt;poetry&lt;/code&gt; for dependency management in python&lt;/strong&gt;&lt;/h1&gt;
&lt;p&gt;In general &lt;code&gt;pip&lt;/code&gt; &amp;amp; &lt;code&gt;venv&lt;/code&gt; is a good combination of tool when you don&amp;rsquo;t have to manage multiple dependencies for your project. But imaging that in a project you need to management multiple dependency files to deploy code into multiple envs. It is possible to do this with &lt;code&gt;pip&lt;/code&gt;, but in that case you need to manage multiple requirements files. To solve this project I have checked a few alternative like  &lt;code&gt;pyenv&lt;/code&gt;, &lt;code&gt;pipx&lt;/code&gt;, &lt;code&gt;pipenv&lt;/code&gt;, &lt;code&gt;poetry&lt;/code&gt; etc. According to my experience, poetry is the simplest and most efficient one. I was checking some of the useful tutorials about this and here I am just taking a note of some of the useful points regarding this tool.&lt;/p&gt;
&lt;h2 id=&#34;__some-useful-poetry-commands__&#34;&gt;&lt;strong&gt;Some useful &lt;code&gt;poetry&lt;/code&gt; commands&lt;/strong&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Download poetry in Ubuntu
curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python -
source $HOME/.poetry/env # Add to PATH
poetry --version # Check version of poetry
poetry self update # Update version
poetry new project1 # Create a new project
cd project1
tree . 
poetry run pytest # Run pytest for the project
poetry add pandas # Add a package as dependency of a project
poetry remove pandas # Delete a project from the file
poetry add --dev pytest # Add a package as dev dependency in a poetry project
poetry add -D coverage[toml] pytest-cov # --dev &amp;amp; -D same
poetry install # Install all the dependencies for a project
poetry build # Build a python library using poetry
poetry publish # Publish library to PyPI
poetry export - requirements.txt --output requirements.txt # Generate requirements.txt
poetry use python3.8 # Use specific version of python in the project
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;__some-important-information__&#34;&gt;&lt;strong&gt;Some important information&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&#34;__important-files__&#34;&gt;&lt;strong&gt;Important files&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;pyproject.toml&lt;/code&gt; is the single file for all project related metadata.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;poetry.lock&lt;/code&gt; file is the granular metadata.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.pypirc&lt;/code&gt; will not work with poetry.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;config.toml&lt;/code&gt; &amp;amp; &lt;code&gt;auth.toml&lt;/code&gt; is used for setting up the artifact repository.&lt;/li&gt;
&lt;li&gt;export &lt;code&gt;POETRY_PYPI_TOKEN_PYPI&lt;/code&gt;, export &lt;code&gt;POETRY_HTTP_BAISC_PYPI_USERNAME&lt;/code&gt; and export &lt;code&gt;POETRY_HTTP_BAISC_PYPI_PASSWORD&lt;/code&gt; can be used for this.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;__publishing-library-as-artifact-to-artifact-store__&#34;&gt;&lt;strong&gt;Publishing library as artifact to artifact store&lt;/strong&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;# config.toml : ~/.config/pypoetry/config.toml
[repositories]
pypi = {url = &amp;quot;https://upload.pypi.org/legacy/&amp;quot;}
testpypi = {url = &amp;quot;https://test.pypi.org/legacy/&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;# auth.toml: ~/.config/pypoetry/auth.toml
[http-basic]
pypi = {username = &amp;quot;myuser&amp;quot;, password = &amp;quot;topsecret&amp;quot;}
testpypi = {username = &amp;quot;myuser&amp;quot;, password = &amp;quot;topsecret&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check GitHub issue related to this &lt;a href=&#34;https://github.com/python-poetry/poetry/issues/111&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;__reference__&#34;&gt;&lt;strong&gt;Reference:&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=G-OAVLBFxbw&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyBites Python Poetry Training&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>SCM conventions &amp; ways of working</title>
      <link>http://localhost:4321/post/scm-conventions-ways-of-working/</link>
      <pubDate>Sun, 31 Oct 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/scm-conventions-ways-of-working/</guid>
      <description>&lt;p&gt;When we are working in an collaborative work environment, this is important to share a common set of convention which makes collaboration easier. Here are some of the common gitops best practice which has helped to collaborate better,&lt;/p&gt;
&lt;h2 id=&#34;__repository-naming-convention__&#34;&gt;&lt;strong&gt;Repository naming convention&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;{short product name}_{component}_{type of repository}&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;__short-product-name__&#34;&gt;&lt;strong&gt;Short product name:&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;modeller:&lt;/strong&gt; modelling engine&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;optimizer:&lt;/strong&gt; optimization engine&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;__component__&#34;&gt;&lt;strong&gt;Component:&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ui:&lt;/strong&gt; any code related to the user interface&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ml:&lt;/strong&gt; anything to do with machine learning code base&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;de:&lt;/strong&gt; anything to do with data engineering code base&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;be:&lt;/strong&gt; anything to do with REST API endpoints for ml, de or in general application.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;__type-of-repository__&#34;&gt;&lt;strong&gt;Type of repository:&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;app:&lt;/strong&gt; mostly used for ui and cli standalone application.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;lib:&lt;/strong&gt; library code base of ml and de engines.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;job:&lt;/strong&gt; cli or script based jobs which will be executed in a batch process.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;api:&lt;/strong&gt; code base related to apis.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;__example__&#34;&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;modeller_ml_lib&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;modeller_de_lib&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;modeller_ml_job&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;modeller_be_api&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;modeller_ui_app&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;__branch-naming-convention__&#34;&gt;&lt;strong&gt;Branch naming convention:&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&#34;__code-flow-branches-restricted-commit-branch__&#34;&gt;&lt;strong&gt;Code Flow Branches (restricted commit branch):&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;develop:&lt;/strong&gt; developers can merge their branches here.
&lt;strong&gt;staging:&lt;/strong&gt; any final tagging or testing has to happen here.
&lt;strong&gt;master:&lt;/strong&gt; production branch, if all the validation is working in staging and tested code needs to be merged here.
&lt;strong&gt;test:&lt;/strong&gt; other than unit testing regression, integration and end to end testing has to happen here.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Flow of code:&lt;/strong&gt; &lt;code&gt;develop &amp;gt; test &amp;gt; staging &amp;gt; production&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;__temporary-branches__&#34;&gt;&lt;strong&gt;Temporary Branches:&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;feature:&lt;/strong&gt; anything which is a feature has to be developed in a feature branch and the branch name should be like : &lt;code&gt;feature/{name of the feature}&lt;/code&gt; [example: &lt;code&gt;feature/integrate-swagger&lt;/code&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;bug fix:&lt;/strong&gt; any bug fix has to be done in a bud fix branch. the structure remains as feature branch. structure of the branch name will be like &lt;code&gt;bugfix/{bug which is being fixed}&lt;/code&gt; [example: &lt;code&gt;bugfix/more-gray-shades-in-loader&lt;/code&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;hot fix:&lt;/strong&gt; any hotfix from master has to be made in this branch. also, note that these fixes has to be merged in other common branches that the master branch. [example: &lt;code&gt;hotfix/disable-endpoint-zero-day-exploit&lt;/code&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;experimental:&lt;/strong&gt; any experimental work has to be tagged this way while creating a branch. [example: &lt;code&gt;experimental/dark-theme-support&lt;/code&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;build:&lt;/strong&gt; any build branch should start with the build pre-fix. [example: &lt;code&gt;build/jacoco-metric&lt;/code&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;release:&lt;/strong&gt; any release branch has to be tagged by a release prefix. [example: &lt;code&gt;release/myapp-1.01.123&lt;/code&gt;]&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;merging:&lt;/strong&gt; any intermediate merge branch has to be tagged with merge prefix. [example: &lt;code&gt;merge/dev_lombok-refactoring&lt;/code&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;__commit-convention__&#34;&gt;&lt;strong&gt;Commit convention:&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;We are adding some keyword to the commit messages based on which a semantic versioning tag can be generated. A version tag will look like, &lt;code&gt;v{MAJOR}.{MINOR}.{PATCH}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Here, this is how commit tags are getting translated to semantic versioning numbers, &lt;code&gt;fix --&amp;gt; patch&lt;/code&gt;, &lt;code&gt;feat --&amp;gt; minor&lt;/code&gt; and &lt;code&gt;BREAKING CHANGE --&amp;gt; major&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;__area-of-work__&#34;&gt;&lt;strong&gt;Area of work:&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;fix:&lt;/strong&gt; if there is a small change which does not break any of the high level APIs and does not change overall behaviors of a feature that will be consider as fix. this corresponds to the &lt;code&gt;patch&lt;/code&gt; in the semantic versioning.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;feat:&lt;/strong&gt; if there is a feature level change then the commit should have feat tagged along with in. in terms of the semantic versioning this corresponds to the minor version change.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;BREAKING CHANGE:&lt;/strong&gt; this is a change where multiple modules and high level APIs are getting changed.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;build:&lt;/strong&gt; These are not linked with semver. any build level changes has to be tagged as build. for example, if we are changing some configuration in &lt;code&gt;setup.py&lt;/code&gt; that should be tagged as build commit.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;chore:&lt;/strong&gt; updating grunt tasks etc; no production code change&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ci:&lt;/strong&gt; change in any of the github workflow or azure pipeline level changes has to be tagged as ci commit.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;docs:&lt;/strong&gt; any changes in wiki documentation or library docstring level changes has to be tagged as &lt;code&gt;docs&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;style:&lt;/strong&gt; if there is any changes in code due to formatting or linting those can be tagged as style change.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;refactor:&lt;/strong&gt; if some code is being refactored that has to be tagged as refactored commit.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;perf:&lt;/strong&gt; for profiling a code base use pref tag.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;test:&lt;/strong&gt; for any commit related to tests can be tagged as &lt;code&gt;test&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;__commit-structure__&#34;&gt;&lt;strong&gt;Commit structure:&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;{area_of_work}: {ticket_id} {One line description of commit}&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;[optional] Detailed description of the commit&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;__in-general-things-related-to-scm__&#34;&gt;&lt;strong&gt;In general things related to SCM:&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Do not push large files in git.&lt;/li&gt;
&lt;li&gt;Do not pass secrets in git.&lt;/li&gt;
&lt;li&gt;Do not directly commit in no commit branches.&lt;/li&gt;
&lt;li&gt;Generate .gitignore file from &lt;code&gt;gitignore.io&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;__reference__&#34;&gt;&lt;strong&gt;Reference:&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://en.linkapi.solutions/blog/conventional-commits-pattern&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Conventional Commits Pattern&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.aritro.in/post/manage-python-library-versioning-using-commitizen-pre-commit-hook/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Python library version management with GitHub action + commitizen + pre-commit hook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.conventionalcommits.org/en/v1.0.0-beta.2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Conventional commits&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Useful tools for docker container management</title>
      <link>http://localhost:4321/post/useful-tools-for-docker-container-management/</link>
      <pubDate>Sun, 31 Oct 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/useful-tools-for-docker-container-management/</guid>
      <description>&lt;p&gt;In this post I&amp;rsquo;m going to discuss three docker tools which I find really useful in my day to day workflow. These tools are,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/wagoodman/dive&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dive&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://portainer.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Portainer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://dozzle.dev/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;dozzle&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;dive-quick-check-of-docker-images&#34;&gt;Dive: quick check of docker images&lt;/h2&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./dive.gif&#34; alt=&#34;Dive&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Dive is super useful to check if there any unnecessary data stored in a docker image. In Ubuntu to install dive run the following commands,&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://github.com/wagoodman/dive/releases/download/v0.9.2/dive_0.9.2_linux_amd64.deb
sudo apt install ./dive_0.9.2_linux_amd64.deb
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note, replace this &lt;code&gt;0.9.2&lt;/code&gt; version to the latest one. Once this tool is installed in your system you can use,&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;dive &amp;lt;your-image-tag&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;dive build -t &amp;lt;some-tag&amp;gt; .
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can use dive as GitHub action in your workflow. Checkout &lt;a href=&#34;https://github.com/marketplace/actions/dive-action&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this&lt;/a&gt; action which helps integrating dive in your workflow using github action.&lt;/p&gt;
&lt;h2 id=&#34;dozzle-a-light-weight-centralized-log-monitoring-tool-for-containers&#34;&gt;Dozzle: a light-weight centralized log monitoring tool for containers&lt;/h2&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./dozzle.gif&#34; alt=&#34;Dozzle&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;This an useful tool which can be used for monitoring all the running containers live logs. This can be used with normal docker-compose or docker swarm.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;dozzle:
  container_name: dozzle
  image: amir20/dozzle:latest
  volumes:
    - /var/run/docker.sock:/var/run/docker.sock
  ports:
    - &amp;quot;8001:8080&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you add the above section in any of the existing docker-compose file this will work out of the box and will show all the running logs of the same docker compose file in &lt;code&gt;localhost:8001&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;portainer-gui-to-monitor-all-running-container&#34;&gt;Portainer: GUI to monitor all running container&lt;/h2&gt;
&lt;p&gt;This tool is good to see all the all docker containers in the system and meta data related to the docker containers.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./portainer.png&#34; alt=&#34;Portainer&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Portainer 2.0 out of the box works with,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;docker&lt;/li&gt;
&lt;li&gt;docker-compose&lt;/li&gt;
&lt;li&gt;docker-swarm&lt;/li&gt;
&lt;li&gt;kubernetes&lt;/li&gt;
&lt;li&gt;azure container registry&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=QBNaOdNSsx8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.youtube.com/watch?v=QBNaOdNSsx8&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/amir20/dozzle&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/amir20/dozzle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.portainer.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://docs.portainer.io/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Python library version management with GitHub action &#43; commitizen &#43; pre-commit hook</title>
      <link>http://localhost:4321/post/manage-python-library-versioning-using-commitizen-pre-commit-hook/</link>
      <pubDate>Sun, 24 Oct 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/manage-python-library-versioning-using-commitizen-pre-commit-hook/</guid>
      <description>&lt;h2 id=&#34;__list-of-tools-used-in-this-post__&#34;&gt;&lt;strong&gt;List of tools used in this post&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://commitizen-tools.github.io/commitizen/#:~:text=Commitizen%20is%20a%20tool%20designed,and%20enforces%20writing%20descriptive%20commits.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;commitizen&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.conventionalcommits.org/en/v1.0.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;conventionalcommits&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://git-scm.com/book/en/v2/Customizing-Git-Git-Hooks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Git Hooks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pre-commit.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pre-commit hook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/features/actions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub Action&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://semver.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Semantic Versioning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is a &lt;a href=&#34;https://github.com/pandalearnstocode/minimal_library_workflow&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sample repository&lt;/a&gt; which show the implementation below.&lt;/p&gt;
&lt;h2 id=&#34;__context--overview__&#34;&gt;&lt;strong&gt;Context &amp;amp; overview&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Lets assume that we are going to build a python library. Lets call it &lt;code&gt;minipackage&lt;/code&gt;. We need to manage the version of the library &amp;amp; need to generate the &lt;code&gt;CHANGELOGS.md&lt;/code&gt; whenever we release a version of the library. There are multiple ways to solve this problem. Here in the post we are going see how we can use semantic versioning to solve this problem.&lt;/p&gt;
&lt;h2 id=&#34;__project-directory-structure__&#34;&gt;&lt;strong&gt;Project directory structure&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;minipackage&lt;/code&gt;: All the source code for the python library will live here.
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;__init.py__&lt;/code&gt;: Library version, other meta data will be here. Also, we need to register any external methods we want to expose as API in the library.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;main.py&lt;/code&gt;: main module is kind of representation of any module and sub-module which will be part of the python library.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pyproject.toml&lt;/code&gt;: This file will have all the meta-data related to the tools which we will be using this project. In the example, &lt;code&gt;tool.commitizen&lt;/code&gt; example is present but there can be many more settings registered in this file related to the other tools being used in the project.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;README.md&lt;/code&gt;: self explanatory.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;requirements.txt&lt;/code&gt;: All the python dependencies required for the library.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.pre-commit-config.yaml&lt;/code&gt;: git pre-commit hooks. Here in this example we have just shown the &lt;code&gt;commitizen&lt;/code&gt; pre-commit hook example. But there can be many other as well.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;setup.py&lt;/code&gt;: all the library building related information will be here.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you want to refresh some basics of python library building process checkout this &lt;a href=&#34;https://www.youtube.com/watch?v=P3dY3uDmnkU&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link&lt;/a&gt;. Regarding &lt;a href=&#34;https://www.youtube.com/watch?v=Wmw-VGSjSNg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pre-commit&lt;/a&gt; checkout this link.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;.
├── minipackage
│   ├── __init__.py
│   ├── main.py
├── pyproject.toml
├── README.md
├── requirements.txt
├── .pre-commit-config.yaml
└── setup.py
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;__pyprojecttoml-project-tools-and-metadata-related-to-tools-used-in-project__&#34;&gt;&lt;strong&gt;&lt;code&gt;pyproject.toml&lt;/code&gt;: project tools and metadata related to tools used in project&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;version&lt;/code&gt; and &lt;code&gt;version_files&lt;/code&gt; these are the two import field which has a major impact in this workflow. make sure that &lt;code&gt;version_files&lt;/code&gt; field points to the file which is the single source of version. We also need to ensure that we will be using the same file and variable all across the project. While start a project &lt;code&gt;__version__&lt;/code&gt; in &lt;code&gt;__init__.py&lt;/code&gt; and &lt;code&gt;version&lt;/code&gt; variable in &lt;code&gt;pyproject.toml&lt;/code&gt; file should match.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;[tool.commitizen]
name = &amp;quot;cz_conventional_commits&amp;quot;
version = &amp;quot;0.0.1&amp;quot;
version_files = [
    &amp;quot;minipackage/__init__.py&amp;quot;,
    &amp;quot;pyproject.toml:version&amp;quot;
]
tag_format = &amp;quot;v$version&amp;quot;
bump_message = &amp;quot;bump: $current_version → $new_version [skip-ci]&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note, here in our example here we using the variable &lt;code&gt;__version__&lt;/code&gt; in all place in the project to manage the version and the same file is mentioned in the pyproject file. Now, when we using &lt;code&gt;commitizen&lt;/code&gt; to bump version of the library it will change value of this variable depending on the commit message tags. Also, this will generate a &lt;code&gt;changelog.md&lt;/code&gt; file in the project root directory and create a release tag.&lt;/p&gt;
&lt;h3 id=&#34;__setuppy-library-build-related-information__&#34;&gt;&lt;strong&gt;&lt;code&gt;setup.py&lt;/code&gt;: library build related information&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;In this section, notice that we using &lt;code&gt;__version__&lt;/code&gt; variable from &lt;code&gt;minipackage&lt;/code&gt; module here. We are passing this &lt;code&gt;__version__&lt;/code&gt; variable to in &lt;code&gt;version&lt;/code&gt; argument in setup function. This will ensure whenever we are building a library, the library gets tagged with this version.&lt;/p&gt;
&lt;p&gt;Note, that whatever is being defined or imported in &lt;code&gt;minipackage/__init__py&lt;/code&gt; file will be present in in &lt;code&gt;minipackage&lt;/code&gt; module.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from setuptools import find_packages, setup
from minipackage import __version__

setup(
    author=&amp;quot;Aritra Biswas&amp;quot;,
    author_email=&amp;quot;pandalearnstocode@gmail.com&amp;quot;,
    python_requires=&amp;quot;&amp;gt;=3.8&amp;quot;,
    install_requires=requirements,
    include_package_data=True,
    keywords=&amp;quot;minipackage&amp;quot;,
    name=&amp;quot;minipackage&amp;quot;,
    packages=find_packages(include=[&amp;quot;minipackage&amp;quot;, &amp;quot;minipackage.*&amp;quot;]),
    version=__version__,
)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;__source__init__py-main-place-where-the-version-variable-is-being-used__&#34;&gt;&lt;strong&gt;&lt;code&gt;source/__init__.py&lt;/code&gt;: main place where the version variable is being used&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;This will be the initial state of the init file. Later when we bump library version in the CI pipeline, this &lt;code&gt;__version__&lt;/code&gt; variable will change depending upon commit message tags.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;__version__ = &amp;quot;0.0.1&amp;quot;
from minipackage.main import hello_world, hello_mcu, hello_dc, 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./cc.png&#34; alt=&#34;Example CZ commits&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;__pre-commit-configyaml--ensuring-commit-message-format-is-being-followed__&#34;&gt;&lt;strong&gt;&lt;code&gt;.pre-commit-config.yaml&lt;/code&gt; : ensuring commit message format is being followed&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;This is not a mandatory thing but kind of a fail safe mechanism to implement conventional commit messages in our day to day workflow.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;repos:
- repo: https://github.com/commitizen-tools/commitizen
  rev: v2.19.0
  hooks:
    - id: commitizen
      stages: [commit-msg]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One this pre-commit hook is install to a repository, whenever we are going to make commit this will check the commit tags are present in the commit message or not. To know more about this in depth go through this &lt;a href=&#34;https://www.conventionalcommits.org/en/v1.0.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;site&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./featured.png&#34; alt=&#34;SemVer&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;__githubworkflowsbumpversionyaml--github-action-to-update-the-version-value-in-the-respective-file__&#34;&gt;&lt;strong&gt;&lt;code&gt;.github/workflows/bumpversion.yaml&lt;/code&gt; : GitHub action to update the version value in the respective file&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;For a commit message like &lt;code&gt;bump: update library version.&lt;/code&gt; to the configured branch, this will update the library version, generate changelog and push it to the feature branch. Post that when we trigger a build, a library with the same version tag will be generated and sent to the python library repository.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;name: NEW Bump version
on:
  push:
    branches:
      - develop
jobs:
  bump_version:
    if: &amp;quot;!startsWith(github.event.head_commit.message, &#39;bump:&#39;)&amp;quot;
    runs-on: ubuntu-latest
    name: &amp;quot;Bump version and create changelog with commitizen&amp;quot;
    steps:
      - name: Check out
        uses: actions/checkout@v2
        with:
          fetch-depth: 0
          token: &amp;quot;${{ secrets.GITHUB_TOKEN }}&amp;quot;
      - id: cz
        name: Create bump and changelog
        uses: commitizen-tools/commitizen-action@master
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          branch: develop
      - name: Print Version
        run: echo &amp;quot;Bumped to version ${{ steps.cz.outputs.version }}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This workflow may change depending upon how you want to update the library version. Here action is driven by push to develop branch but it is possible to setup this process with pull request trigger or any other trigger as well.&lt;/p&gt;
&lt;h3 id=&#34;__reference__&#34;&gt;&lt;strong&gt;Reference:&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://en.linkapi.solutions/blog/conventional-commits-pattern&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Conventional Commits Pattern&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Local development environment with Fast API &#43; SQLModel &#43; SQLite &#43; Alembic</title>
      <link>http://localhost:4321/post/local-development-environment-with-fast-api-sqlmodel-sqlite-alembic-sync-async-version/</link>
      <pubDate>Sat, 16 Oct 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/local-development-environment-with-fast-api-sqlmodel-sqlite-alembic-sync-async-version/</guid>
      <description>&lt;p&gt;This is important to have a working local development environment for quick prototyping and developing CRUD application using Fast API. Here in this post we are going to explore how we can quickly spin up and a local development environment. Here we are going to use the following tools to develop the REST API endpoints,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://fastapi.tiangolo.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fast API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sqlmodel.tiangolo.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SQLModel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.sqlite.org/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SQLite&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://alembic.sqlalchemy.org/en/latest/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Alembic&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;prerequisite&#34;&gt;Prerequisite:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.anaconda.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Anaconda&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sqlitebrowser.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DB browser for SQLite&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;curl&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;code-repository&#34;&gt;Code repository:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/pandalearnstocode/fastapi_sqlmodel_sqlite_alembic_sync_template&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fast API + SQLite + SQLModel + Alembic (Sync version)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/pandalearnstocode/fastapi_sqlmodel_sqlite_alembic_async_template&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fast API + SQLite + SQLModel + Alembic (Async version)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;sample-api-workflow&#34;&gt;Sample API workflow:&lt;/h3&gt;
&lt;h4 id=&#34;workflow&#34;&gt;Workflow:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Write database models&lt;/li&gt;
&lt;li&gt;Write database settings&lt;/li&gt;
&lt;li&gt;Write REST API endpoints&lt;/li&gt;
&lt;li&gt;Test APIs&lt;/li&gt;
&lt;li&gt;Setup alembic&lt;/li&gt;
&lt;li&gt;Change database models&lt;/li&gt;
&lt;li&gt;Update APIs&lt;/li&gt;
&lt;li&gt;Run alembic migration&lt;/li&gt;
&lt;li&gt;Test APIs&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;project-structure&#34;&gt;Project structure:&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;.
├── app
│   ├── db.py : All the settings related to DB will be here.
│   ├── __init__.py
│   ├── main.py : All endpoints will be defined here.
│   └── models.py : All the data models will be defined here.
└── database.db : This SQLite DB will be created and data will be stored here.
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;step-1-setup-development-environment&#34;&gt;Step 1: Setup development environment&lt;/h3&gt;
&lt;p&gt;Create a local development conda environment to run the application from local. Here we are going to use python 3.8. After creating the environment install the required dependencies.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create --name fastapi_sqlmodel python=3.8
conda activate fastapi_sqlmodel
pip install fastapi[all] sqlmodel alembic aiosqlite
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;step-2-create-database-models-in-modelspy&#34;&gt;Step 2: Create database models in &lt;code&gt;models.py&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Here we are going to define a database model to store information related to &lt;code&gt;task name&lt;/code&gt;. Task table is the table which will be updated later with the &lt;code&gt;task description&lt;/code&gt; information and we will perform a migration to see how the changes are going to be maintained and reflected in the database.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sqlmodel import SQLModel, Field

class TaskBase(SQLModel):
    task_name: str

class Task(TaskBase, table=True):
    id: int = Field(default=None, primary_key=True)

class TaskCreate(TaskBase):
    pass
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;step-3-create-settings-related-to-db-in-dbpy&#34;&gt;Step 3: Create settings related to DB in &lt;code&gt;db.py&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Here for rapid prototype we are going to use sqlite local db. Which is kind of a standalone file. To view the record in the file we can we use the &lt;a href=&#34;https://sqlitebrowser.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DB browser for SQLite&lt;/a&gt;. After we run the application a a file called &lt;code&gt;database.db&lt;/code&gt; will be created in the root directory of the project.&lt;/p&gt;
&lt;h4 id=&#34;sync-version&#34;&gt;Sync version:&lt;/h4&gt;
&lt;p&gt;Here are going to create two functions which will create all the required tables when the FastAPI application starts and will generate a session using which I/O operations will be performed in a DB.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sqlmodel import Session, SQLModel, create_engine
sqlite_file_name = &amp;quot;database.db&amp;quot;
sqlite_url = f&amp;quot;sqlite:///{sqlite_file_name}&amp;quot;

connect_args = {&amp;quot;check_same_thread&amp;quot;: False}
engine = create_engine(sqlite_url, echo=True, connect_args=connect_args)

def create_db_and_tables():
    SQLModel.metadata.create_all(engine)

def get_session():
    with Session(engine) as session:
        yield session
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;async-version&#34;&gt;Async version:&lt;/h4&gt;
&lt;p&gt;Asyn version of the above defined functions which will be used in &lt;code&gt;main.py&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sqlmodel import SQLModel
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
from sqlalchemy.orm import sessionmaker

sqlite_file_name = &amp;quot;database.db&amp;quot;
sqlite_url = f&amp;quot;sqlite+aiosqlite:///{sqlite_file_name}&amp;quot;
engine = create_async_engine(sqlite_url, echo=True, future=True)

async def init_db():
    async with engine.begin() as conn:
        await conn.run_sync(SQLModel.metadata.create_all)

async def get_session() -&amp;gt; AsyncSession:
    async_session = sessionmaker(
        engine, class_=AsyncSession, expire_on_commit=False
    )
    async with async_session() as session:
        yield session
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;step-4-define-api-endpoints-in-mainpy&#34;&gt;Step 4: Define API endpoints in &lt;code&gt;main.py&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Here we are going to define a POST endpoint using which we can write some task related information in the database. As of now, the request model for this endpoint is &lt;code&gt;TaskCreate&lt;/code&gt; and in the response this endpoint will return an object of type &lt;code&gt;Task&lt;/code&gt;.&lt;/p&gt;
&lt;h4 id=&#34;sync-version-1&#34;&gt;Sync version:&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from fastapi import FastAPI, Depends
from sqlmodel import Session
from app.db import create_db_and_tables, get_session
from app.models import Task, TaskCreate

app = FastAPI()

@app.on_event(&amp;quot;startup&amp;quot;)
def on_startup():
    create_db_and_tables()

@app.get(&amp;quot;/ping&amp;quot;)
def pong():
    return {&amp;quot;ping&amp;quot;: &amp;quot;pong!&amp;quot;}

@app.post(&amp;quot;/task/&amp;quot;, response_model=Task)
def create_task(task: TaskCreate, session: Session = Depends(get_session)):
    db_task = Task.from_orm(task)
    session.add(db_task)
    session.commit()
    session.refresh(db_task)
    return db_task
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;async-version-1&#34;&gt;Async version:&lt;/h4&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from fastapi import FastAPI, Depends
from sqlmodel import Session
from app.db import create_db_and_tables, get_session
from app.models import Task, TaskCreate

app = FastAPI()

@app.on_event(&amp;quot;startup&amp;quot;)
def on_startup():
    create_db_and_tables()

@app.get(&amp;quot;/ping&amp;quot;)
def pong():
    return {&amp;quot;ping&amp;quot;: &amp;quot;pong!&amp;quot;}

@app.post(&amp;quot;/task/&amp;quot;, response_model=Task)
def create_task(task: TaskCreate, session: Session = Depends(get_session)):
    db_task = Task.from_orm(task)
    session.add(db_task)
    session.commit()
    session.refresh(db_task)
    return db_task
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;step-5-test-api-endpoints&#34;&gt;Step 5: Test API endpoints&lt;/h3&gt;
&lt;p&gt;Here we are going to test the API endpoints are working fine or not. If we get the expected result from the CURL and can validate the records are being updated in the DB we can expect that the APIs are working fine.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;uvicorn app.main:app --reload
curl -X POST http://127.0.0.1:8000/task/ -H &#39;accept: application/json&#39; -H &#39;Content-Type: application/json&#39; -d &#39;{&amp;quot;task_name&amp;quot;: &amp;quot;just added task&amp;quot;}&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a &lt;code&gt;database.db&lt;/code&gt; file will be create in project root directory.&lt;/li&gt;
&lt;li&gt;after making the post call validate the db records are being updated using &lt;code&gt;DB browser for SQLite&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;step-6-setup-alembic&#34;&gt;Step 6: Setup alembic&lt;/h3&gt;
&lt;h4 id=&#34;step-6a-generate-alembic-settings&#34;&gt;Step 6a: Generate alembic settings&lt;/h4&gt;
&lt;p&gt;In this step we are going generate all the settings for alembic migration. This will generate some files and folders in the project root directory.&lt;/p&gt;
&lt;h5 id=&#34;sync-version-2&#34;&gt;Sync version:&lt;/h5&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;alembic init alembic
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;async-version-2&#34;&gt;Async version:&lt;/h5&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;alembic init -t async alembic
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;step-6b-update-alembic-settings&#34;&gt;Step 6b: Update alembic settings&lt;/h4&gt;
&lt;p&gt;After these files are generated, change the DB url in &lt;code&gt;alembic.ini&lt;/code&gt;.&lt;/p&gt;
&lt;h5 id=&#34;sync-version-3&#34;&gt;Sync version:&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;replace &lt;code&gt;sqlalchemy.url = driver://user:pass@localhost/dbname&lt;/code&gt; in &lt;code&gt;alembic.ini&lt;/code&gt; with &lt;code&gt;sqlite:///database.db&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;async-version-3&#34;&gt;Async version:&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;replace &lt;code&gt;sqlalchemy.url = driver://user:pass@localhost/dbname&lt;/code&gt; in &lt;code&gt;alembic.ini&lt;/code&gt; with &lt;code&gt;sqlite+aiosqlite:///database.db&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;common-changes-for-sync--async-version&#34;&gt;Common changes for Sync &amp;amp; Async version:&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;in &lt;code&gt;alembic/env.py&lt;/code&gt; file add &lt;code&gt;from sqlmodel import SQLModel&lt;/code&gt; in the import section.&lt;/li&gt;
&lt;li&gt;in &lt;code&gt;alembic/env.py&lt;/code&gt; file add the following line in import section, &lt;code&gt;from app.models import Task&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;in &lt;code&gt;alembic/env.py&lt;/code&gt; change &lt;code&gt;target_metadata = None&lt;/code&gt; to &lt;code&gt;target_metadata = SQLModel.metadata&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;in &lt;code&gt;alembic/script.py.mako&lt;/code&gt; add &lt;code&gt;import sqlmodel&lt;/code&gt; in the import section.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;step-6c-generate-alembic-migration-settings&#34;&gt;Step 6c: Generate alembic migration settings&lt;/h4&gt;
&lt;p&gt;After making all the changes, make first migration.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;alembic revision --autogenerate -m &amp;quot;init&amp;quot;
alembic upgrade head
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;step-7-update-data-models-in-modelspy&#34;&gt;Step 7: Update data models in &lt;code&gt;models.py&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Now, lets say there is some changes we need to add to our data model. In this case we are just going to add &lt;code&gt;task_description&lt;/code&gt;. &lt;code&gt;task_description&lt;/code&gt; will be a optional string column in the &lt;code&gt;Task&lt;/code&gt; table. After we make the required changes in the &lt;code&gt;app/models.py&lt;/code&gt; and &lt;code&gt;app/main.py&lt;/code&gt;, we need to run the migration to add the newly added columns in the DB.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sqlmodel import SQLModel, Field
from typing import Optional # This is a new add line

class TaskBase(SQLModel):
    task_name: str
    task_description: Optional[str] = None # This is a new add line

class Task(TaskBase, table=True):
    id: int = Field(default=None, primary_key=True)

class TaskCreate(TaskBase):
    pass
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;step-8-update-api-endpoints-in-mainpy&#34;&gt;Step 8: Update API endpoints in &lt;code&gt;main.py&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;In this case, endpoints remains unchanged but in a realistic scenario endpoints has to be changed to take the new information into account.&lt;/p&gt;
&lt;h3 id=&#34;step-9-run-db-migration&#34;&gt;Step 9: Run DB migration&lt;/h3&gt;
&lt;p&gt;Run the following line to update the database tables.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;alembic revision --autogenerate -m &amp;quot;add description&amp;quot;
alembic upgrade head
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;step-10-test-updated-api-endpoints&#34;&gt;Step 10: Test updated API endpoints&lt;/h3&gt;
&lt;p&gt;As last step, make a call to the API endpoints which will be using the updated table and check if everything is working fine or not.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;uvicorn app.main:app --reload
curl -X POST http://127.0.0.1:8000/task/ -H &#39;accept: application/json&#39; -H &#39;Content-Type: application/json&#39; -d &#39;{&amp;quot;task_name&amp;quot;: &amp;quot;just added task&amp;quot;,&amp;quot;task_description&amp;quot;:&amp;quot;a newly created task&amp;quot;}&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;after making the post call validate the db records are being updated using &lt;code&gt;DB browser for SQLite&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion:&lt;/h3&gt;
&lt;p&gt;This workflow is quite good when in come to rapid prototyping. One can use this inside a docker container as well but make sure that the volume mounting is in place to keep the standalone database file and the other related alembic migration scripts.&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://testdriven.io/blog/fastapi-sqlmodel/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FastAPI with Async SQLAlchemy, SQLModel, and Alembic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://fastapi.tiangolo.com/advanced/async-sql-databases/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Async SQL (Relational) Databases&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/testdrivenio/fastapi-sqlmodel-alembic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FastAPI + SQLModel + Alembic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://fastapi.tiangolo.com/tutorial/sql-databases/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SQL (Relational) Databases&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://python.plainenglish.io/building-a-phone-directory-with-mysql-fastapi-and-angular-cd48673904f4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Building a Phone Directory with Python, MySQL, FastAPI, and Angular&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://alembic.sqlalchemy.org/en/latest/autogenerate.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Alembic: Auto Generating Migrations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/build-an-async-python-service-with-fastapi-sqlalchemy-196d8792fa08&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Build an async python service with FastAPI &amp;amp; SQLAlchemy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://hackernoon.com/how-to-set-up-fastapi-ormar-and-alembic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How to set up FastAPI, Ormar, and Alembic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/tiangolo/uvicorn-gunicorn-fastapi-docker&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tiangolo/uvicorn-gunicorn-fastapi-docker&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
